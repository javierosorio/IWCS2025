{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300834a9-726c-4c32-b1cb-181fecba5e05",
   "metadata": {},
   "source": [
    "Jaccard Distance measures how many words two sentences share, ignoring word order. It ranges from 0 (all words the same) to 1 (no words in common). We are using it to evaluate how well the Lexical Divergence score reflects actual word overlap.\n",
    "By comparing the two scores, we can check how much they agree. A small difference means both detect similar changes. A large difference shows they respond to different types of variation (like word order vs word content). This helps us assess the reliability of the Lexical Divergence model.\n",
    "91.94% accuracy for the lexicaldev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a076da61-3305-46de-8f5e-315e38d23cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\brike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# b.l. import libraries for data, text processing, and nltk tools\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#b.l. using the same preprocessing tools as Afraa, download stopwords from nltk \n",
    "nltk.download('stopwords')\n",
    "\n",
    "# b.l. load the file with aligned sentence pairs that has the results of the lexdiv\n",
    "df = pd.read_csv(\"lexical_divergence_results.csv\")\n",
    "\n",
    "# b.l. get the list of english stopwords from nltk\n",
    "nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# b.l. define a tokenizer similar to the one used in lexical divergence\n",
    "def match_original_tokenizer(text):\n",
    "    tokens = re.findall(r'\\b[a-z]+\\b', text.lower())\n",
    "    return [w for w in tokens if w not in nltk_stop_words]\n",
    "\n",
    "# b.l. create a list to store jaccard distance values\n",
    "jaccard_distances = []\n",
    "\n",
    "#b.l. for each row, calculate jaccard distance between token sets\n",
    "for _, row in df.iterrows():\n",
    "    src_tokens = set(match_original_tokenizer(row['Original_EN']))\n",
    "    mt_tokens = set(match_original_tokenizer(row['MT_EN']))\n",
    "    intersection = src_tokens.intersection(mt_tokens)\n",
    "    union = src_tokens.union(mt_tokens)\n",
    "    jaccard = 1 - len(intersection) / len(union) if union else 0\n",
    "    jaccard_distances.append(round(jaccard, 4))\n",
    "\n",
    "# b.l. add the new score as a column to the dataframe\n",
    "df['Jaccard_Distance'] = jaccard_distances\n",
    "\n",
    "# b.l. calculate the absolute difference between lexical divergence and jaccard distance\n",
    "df['Divergence_Jaccard_Diff'] = (df['Lexical_Divergence'] - df['Jaccard_Distance']).abs()\n",
    "\n",
    "# b.l. save the updated file with both new columns\n",
    "df.to_csv(\"lexical_divergence_with_jaccard.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae4141-4cff-411b-90a9-995160b68606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
