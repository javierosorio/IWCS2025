{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHmII-yTtrqF",
        "outputId": "dc451bfc-2669-4431-b3b6-257e47588fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Core NLP and ML libraries\n",
        "!pip install -q pandas numpy tqdm scikit-learn matplotlib seaborn fpdf\n",
        "\n",
        "# Stanza for tokenization & lemmatization\n",
        "!pip install -q stanza\n",
        "\n",
        "# Sentence-Transformers (for ConfliBERT)\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "# HuggingFace Transformers for tokenizer compatibility\n",
        "!pip install -q transformers\n",
        "\n",
        "# spaCy for POS tagging (optional, but included if needed later)\n",
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "# NLTK for WordNet and lemmatization\n",
        "!pip install -q nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"punkt_tab\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toMJSlA0COyr",
        "outputId": "9248940c-2809-432f-f9f4-99e11741be89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Final Lexical Divergence Evaluation Script (Corrected Divergence Metric) ===\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from difflib import SequenceMatcher\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# === Setup Paths ===\n",
        "DATA_PATH = \"/content/drive/MyDrive/Summer/CASS\"\n",
        "OUTPUT_PATH = os.path.join(DATA_PATH, \"output_lexical_gmm\")\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "INPUT_PAIRS = os.path.join(DATA_PATH, \"sentence_pairs.csv\")\n",
        "OUTPUT_MASTER = os.path.join(OUTPUT_PATH, \"lexical_divergence_results.csv\")\n",
        "OUTPUT_HIST = os.path.join(OUTPUT_PATH, \"lexical_divergence_distribution.png\")\n",
        "OUTPUT_SUMMARY = os.path.join(OUTPUT_PATH, \"lexical_divergence_summary.csv\")\n",
        "OUTPUT_FLAG_PLOT = os.path.join(OUTPUT_PATH, \"lexical_flag_distribution.png\")\n",
        "OUTPUT_CLASS_PLOT = os.path.join(OUTPUT_PATH, \"lexical_class_comparison.png\")\n",
        "\n",
        "# === NLTK Setup ===\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# === Load Data ===\n",
        "df_pairs = pd.read_csv(INPUT_PAIRS)\n",
        "\n",
        "# === Calculate Lexical Divergence ===\n",
        "records = []\n",
        "for idx, row in df_pairs.iterrows():\n",
        "    sid = f\"S{idx+1:03d}\"\n",
        "    src_tokens = [lemmatizer.lemmatize(w.lower()) for w in nltk.word_tokenize(row['Original_EN']) if w.isalpha() and w.lower() not in stop_words]\n",
        "    mt_tokens = [lemmatizer.lemmatize(w.lower()) for w in nltk.word_tokenize(row['MT_EN']) if w.isalpha() and w.lower() not in stop_words]\n",
        "    matcher = SequenceMatcher(None, src_tokens, mt_tokens)\n",
        "    match_len = sum(block.size for block in matcher.get_matching_blocks())\n",
        "    divergence = 1 - (match_len / max(len(src_tokens), len(mt_tokens), 1))\n",
        "    records.append({\n",
        "        \"Sentence_ID\": sid,\n",
        "        \"Original_EN\": row['Original_EN'],\n",
        "        \"MT_EN\": row['MT_EN'],\n",
        "        \"Lexical_Divergence\": round(divergence, 4)\n",
        "    })\n",
        "\n",
        "# === Save Raw Output ===\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "# === GMM Thresholding ===\n",
        "gmm = GaussianMixture(n_components=2).fit(df['Lexical_Divergence'].values.reshape(-1, 1))\n",
        "means = gmm.means_.flatten()\n",
        "thresh_gmm = np.mean(means)\n",
        "\n",
        "# === Static Threshold ===\n",
        "thresh_static = 0.35\n",
        "\n",
        "df['Classification_GMM'] = df['Lexical_Divergence'].apply(lambda x: 'Acceptable' if x <= thresh_gmm else 'Divergence')\n",
        "df['Classification_Static'] = df['Lexical_Divergence'].apply(lambda x: 'Acceptable' if x <= thresh_static else 'Divergence')\n",
        "\n",
        "df['Explanation_Flag'] = df['Lexical_Divergence'].apply(\n",
        "    lambda x: 'HighDivergence' if x > 0.6 else ('Moderate' if x > 0.35 else 'Low'))\n",
        "\n",
        "# === Save Output ===\n",
        "df.to_csv(OUTPUT_MASTER, index=False)\n",
        "\n",
        "# === Save Summary Stats ===\n",
        "summary = {\n",
        "    \"Total Sentences\": len(df),\n",
        "    \"Acceptable (GMM)\": (df['Classification_GMM'] == 'Acceptable').sum(),\n",
        "    \"Divergence (GMM)\": (df['Classification_GMM'] == 'Divergence').sum(),\n",
        "    \"Acceptable (Static)\": (df['Classification_Static'] == 'Acceptable').sum(),\n",
        "    \"Divergence (Static)\": (df['Classification_Static'] == 'Divergence').sum(),\n",
        "    \"GMM Threshold Used\": round(thresh_gmm, 3),\n",
        "    \"Static Threshold Used\": round(thresh_static, 3)\n",
        "}\n",
        "pd.DataFrame([summary]).to_csv(OUTPUT_SUMMARY, index=False)\n",
        "\n",
        "# === Plot Lexical Divergence Distribution ===\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['Lexical_Divergence'], bins=20, kde=True, color='skyblue')\n",
        "plt.axvline(thresh_gmm, color='red', linestyle='--', label=f'GMM Threshold = {thresh_gmm:.2f}')\n",
        "plt.axvline(thresh_static, color='green', linestyle='--', label=f'Static Threshold = {thresh_static:.2f}')\n",
        "plt.title(\"Lexical Divergence Score Distribution\")\n",
        "plt.xlabel(\"Lexical Divergence\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.savefig(OUTPUT_HIST)\n",
        "plt.close()\n",
        "\n",
        "# === Plot Classification Count ===\n",
        "plt.figure(figsize=(8, 6))\n",
        "class_data = pd.DataFrame({\n",
        "    'GMM': df['Classification_GMM'].value_counts(),\n",
        "    'Static': df['Classification_Static'].value_counts()\n",
        "}).T\n",
        "class_data[['Acceptable', 'Divergence']].plot(kind='bar', figsize=(8, 5), color=['skyblue', 'salmon'])\n",
        "plt.title(\"Acceptable vs Divergence (GMM vs Static)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_CLASS_PLOT)\n",
        "plt.close()\n",
        "\n",
        "# === Plot Explanation Flags ===\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, x='Explanation_Flag', order=df['Explanation_Flag'].value_counts().index, palette='muted')\n",
        "plt.title(\"Top Lexical Divergence Explanation Flags\")\n",
        "plt.xlabel(\"Flag\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_FLAG_PLOT)\n",
        "plt.close()\n",
        "\n",
        "print(f\"✅ Final Lexical Divergence Evaluation Completed\")\n",
        "print(f\"→ Score Distribution: {OUTPUT_HIST}\")\n",
        "print(f\"→ Summary Saved: {OUTPUT_SUMMARY}\")\n",
        "print(f\"→ Output File: {OUTPUT_MASTER}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "Y_VAK3z8tsYY",
        "outputId": "412b82a3-2319-4351-82cd-3c8251454597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Final Lexical Divergence Evaluation Completed\n",
            "→ Score Distribution: /content/drive/MyDrive/Summer/CASS/output_lexical_gmm/lexical_divergence_distribution.png\n",
            "→ Summary Saved: /content/drive/MyDrive/Summer/CASS/output_lexical_gmm/lexical_divergence_summary.csv\n",
            "→ Output File: /content/drive/MyDrive/Summer/CASS/output_lexical_gmm/lexical_divergence_results.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-694a1130b360>:111: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.countplot(data=df, x='Explanation_Flag', order=df['Explanation_Flag'].value_counts().index, palette='muted')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Lexical Divergence Visualization and Summary Statistics ===\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# === Paths ===\n",
        "DATA_PATH = \"/content/drive/MyDrive/Summer/CASS/output_lexical_gmm\"\n",
        "INPUT_CSV = os.path.join(DATA_PATH, \"lexical_divergence_results.csv\")\n",
        "SUMMARY_CSV = os.path.join(DATA_PATH, \"lexical_divergence_summary.csv\")\n",
        "\n",
        "# === Load Data ===\n",
        "cand_df = pd.read_csv(INPUT_CSV)\n",
        "summary_df = pd.read_csv(SUMMARY_CSV)\n",
        "# Load both thresholds from the summary DataFrame\n",
        "gmm_threshold = summary_df['GMM Threshold Used'].iloc[0]\n",
        "static_threshold = summary_df['Static Threshold Used'].iloc[0]\n",
        "\n",
        "\n",
        "# === Plot 1: Distribution of Lexical Divergence ===\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Correct the column name to match what was saved in the main output CSV\n",
        "sns.histplot(cand_df['Lexical_Divergence'], bins=20, kde=True, color='skyblue')\n",
        "\n",
        "# Add both vertical lines to the plot\n",
        "plt.axvline(gmm_threshold, color='red', linestyle='--', label=f'GMM Threshold = {gmm_threshold:.2f}')\n",
        "plt.axvline(static_threshold, color='green', linestyle='--', label=f'Static Threshold = {static_threshold:.2f}')\n",
        "\n",
        "plt.title(\"Lexical Divergence Score Distribution with GMM and Static Thresholds\") # Update title\n",
        "plt.xlabel(\"Lexical Divergence\") # Update label\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "# Update output filename for consistency\n",
        "plt.savefig(os.path.join(DATA_PATH, \"lexical_divergence_distribution_dual_threshold.png\")) # New filename\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# === Plot 2: Classification Comparison (GMM vs Static) ===\n",
        "# Note: The static threshold logic here uses 0.62, which differs from 0.35 used in the previous cell.\n",
        "# This might be intentional, but worth noting.\n",
        "# Correct the column name for Lexical Divergence\n",
        "cand_df['Classification_Static'] = cand_df['Lexical_Divergence'].apply(lambda x: 'Acceptable' if x >= 0.62 else 'Divergence')\n",
        "gmm_counts = cand_df['Classification_GMM'].value_counts()\n",
        "static_counts = cand_df['Classification_Static'].value_counts()\n",
        "\n",
        "class_df = pd.DataFrame({'GMM': gmm_counts, 'Static': static_counts}).T\n",
        "class_df[['Acceptable', 'Divergence']].plot(kind='bar', color=['green', 'red'])\n",
        "plt.title(\"Acceptable vs Divergence (GMM vs Static)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(DATA_PATH, \"classification_comparison.png\"))\n",
        "plt.close()\n",
        "\n",
        "# === Plot 3: Boxplot of Lexical Score by GMM Classification ===\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Correct the column name for Lexical Divergence\n",
        "sns.boxplot(x='Classification_GMM', y='Lexical_Divergence', data=cand_df, palette='Set2')\n",
        "plt.title(\"Lexical Divergence Distribution by Classification (GMM)\") # Update title\n",
        "plt.ylabel(\"Lexical Divergence\") # Update label\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(DATA_PATH, \"boxplot_gmm_classification.png\"))\n",
        "plt.close()\n",
        "\n",
        "# === Plot 4: Top Divergence Reasons (from Explanation_GMM) ===\n",
        "# Correct the column name for Explanation flags\n",
        "# Note: In the previous cell, the column is named 'Explanation_Flag'.\n",
        "# If you intended to use a GMM-specific explanation, you would need to add that logic in the previous cell.\n",
        "# Assuming 'Explanation_Flag' is the intended column here.\n",
        "top_flags = cand_df['Explanation_Flag'].value_counts().nlargest(10)\n",
        "top_flags.to_csv(os.path.join(DATA_PATH, \"top_explanation_flags.csv\"))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "top_flags.plot(kind='bar', color='purple')\n",
        "plt.title(\"Top Lexical Divergence Explanation Flags\") # Update title\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(DATA_PATH, \"top_explanation_flags.png\"))\n",
        "plt.close()\n",
        "\n",
        "print(\"✅ Lexical Divergence Visualizations & Stats Saved to:\", DATA_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ExXLuThVy_a",
        "outputId": "d0ef2a6c-fa4f-470a-d053-ae981e6983a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-51138f1492f5>:60: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(x='Classification_GMM', y='Lexical_Divergence', data=cand_df, palette='Set2')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Lexical Divergence Visualizations & Stats Saved to: /content/drive/MyDrive/Summer/CASS/output_lexical_gmm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V7cky9LxwkwU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}