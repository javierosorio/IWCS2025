{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VeM324jitXeI",
    "outputId": "abaa2ca0-716b-4756-eaa9-73af6212d19d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
      "     ---------------------------------------- 0.0/587.7 MB ? eta -:--:--\n",
      "     --------------------------------------- 2.1/587.7 MB 16.8 MB/s eta 0:00:35\n",
      "      ------------------------------------- 11.3/587.7 MB 35.2 MB/s eta 0:00:17\n",
      "     - ------------------------------------ 22.8/587.7 MB 42.5 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 31.2/587.7 MB 42.1 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 40.4/587.7 MB 43.5 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 49.0/587.7 MB 42.2 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 58.2/587.7 MB 42.6 MB/s eta 0:00:13\n",
      "     ---- --------------------------------- 67.4/587.7 MB 43.4 MB/s eta 0:00:12\n",
      "     ---- --------------------------------- 75.8/587.7 MB 42.8 MB/s eta 0:00:12\n",
      "     ----- -------------------------------- 87.0/587.7 MB 44.1 MB/s eta 0:00:12\n",
      "     ------ ------------------------------- 98.3/587.7 MB 44.8 MB/s eta 0:00:11\n",
      "     ------ ------------------------------ 109.6/587.7 MB 45.7 MB/s eta 0:00:11\n",
      "     ------- ----------------------------- 119.5/587.7 MB 46.0 MB/s eta 0:00:11\n",
      "     -------- ---------------------------- 130.3/587.7 MB 46.5 MB/s eta 0:00:10\n",
      "     -------- ---------------------------- 140.8/587.7 MB 46.6 MB/s eta 0:00:10\n",
      "     --------- --------------------------- 150.5/587.7 MB 46.7 MB/s eta 0:00:10\n",
      "     ---------- -------------------------- 159.9/587.7 MB 46.4 MB/s eta 0:00:10\n",
      "     ---------- -------------------------- 170.7/587.7 MB 46.6 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 180.9/587.7 MB 46.6 MB/s eta 0:00:09\n",
      "     ------------ ------------------------ 190.8/587.7 MB 46.6 MB/s eta 0:00:09\n",
      "     ------------ ------------------------ 199.2/587.7 MB 46.5 MB/s eta 0:00:09\n",
      "     ------------- ----------------------- 208.7/587.7 MB 46.3 MB/s eta 0:00:09\n",
      "     ------------- ----------------------- 216.3/587.7 MB 45.8 MB/s eta 0:00:09\n",
      "     -------------- ---------------------- 225.4/587.7 MB 45.8 MB/s eta 0:00:08\n",
      "     -------------- ---------------------- 235.4/587.7 MB 45.7 MB/s eta 0:00:08\n",
      "     --------------- --------------------- 244.3/587.7 MB 45.7 MB/s eta 0:00:08\n",
      "     --------------- --------------------- 253.5/587.7 MB 45.7 MB/s eta 0:00:08\n",
      "     ---------------- -------------------- 262.1/587.7 MB 45.5 MB/s eta 0:00:08\n",
      "     ----------------- ------------------- 273.4/587.7 MB 46.3 MB/s eta 0:00:07\n",
      "     ----------------- ------------------- 281.3/587.7 MB 45.8 MB/s eta 0:00:07\n",
      "     ------------------ ------------------ 289.7/587.7 MB 45.8 MB/s eta 0:00:07\n",
      "     ------------------ ------------------ 296.2/587.7 MB 45.5 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 302.0/587.7 MB 44.6 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 310.4/587.7 MB 44.6 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 317.2/587.7 MB 44.2 MB/s eta 0:00:07\n",
      "     -------------------- ---------------- 324.3/587.7 MB 44.0 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 334.2/587.7 MB 44.0 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 342.6/587.7 MB 43.8 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 352.3/587.7 MB 43.4 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 360.2/587.7 MB 43.1 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 367.3/587.7 MB 42.4 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 373.8/587.7 MB 42.1 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 383.8/587.7 MB 42.0 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 391.6/587.7 MB 41.5 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 399.8/587.7 MB 41.1 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 407.6/587.7 MB 40.9 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 417.1/587.7 MB 40.9 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 423.9/587.7 MB 40.5 MB/s eta 0:00:05\n",
      "     --------------------------- --------- 431.8/587.7 MB 40.3 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 438.8/587.7 MB 40.0 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 445.9/587.7 MB 39.6 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 455.1/587.7 MB 39.5 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 464.3/587.7 MB 39.5 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 472.4/587.7 MB 39.4 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 480.0/587.7 MB 39.3 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 489.9/587.7 MB 39.4 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 497.8/587.7 MB 39.2 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 507.0/587.7 MB 39.3 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 515.6/587.7 MB 39.3 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 523.0/587.7 MB 39.1 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 532.2/587.7 MB 38.8 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 541.1/587.7 MB 38.8 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 546.8/587.7 MB 38.8 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 556.3/587.7 MB 38.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 565.2/587.7 MB 39.3 MB/s eta 0:00:01\n",
      "     ------------------------------------  572.5/587.7 MB 39.3 MB/s eta 0:00:01\n",
      "     ------------------------------------  580.4/587.7 MB 39.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  585.6/587.7 MB 39.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  587.5/587.7 MB 39.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  587.5/587.7 MB 39.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  587.5/587.7 MB 39.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  587.5/587.7 MB 39.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  587.5/587.7 MB 39.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  587.5/587.7 MB 39.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- 587.7/587.7 MB 33.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from en-core-web-lg==3.7.1) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2025.1.31)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\brike\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\brike\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.0)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n",
      "Requirement already satisfied: langid in c:\\users\\brike\\anaconda3\\lib\\site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\brike\\anaconda3\\lib\\site-packages (from langid) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "# Core NLP and ML libraries\n",
    "!pip install -q pandas numpy tqdm scikit-learn matplotlib seaborn fpdf\n",
    "\n",
    "# Stanza for tokenization & lemmatization\n",
    "!pip install -q stanza\n",
    "\n",
    "# Sentence-Transformers (for ConfliBERT)\n",
    "!pip install -q sentence-transformers\n",
    "\n",
    "# HuggingFace Transformers for tokenizer compatibility\n",
    "!pip install -q transformers\n",
    "\n",
    "# spaCy for POS tagging (optional, but included if needed later)\n",
    "!pip install -q spacy\n",
    "!python -m spacy download en_core_web_lg\n",
    "\n",
    "# NLTK for WordNet and lemmatization\n",
    "!pip install -q nltk\n",
    "\n",
    "!pip install -q unidecode\n",
    "#!pip install unidecode\n",
    "\n",
    "!pip install langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9-3T6b6ul3-",
    "outputId": "ebac3447-25d8-4988-8012-d6a47fc61458"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\brike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\brike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\brike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\brike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\brike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e3de94d87ea24c1696136b571abc9c11",
      "24e4ca6f90ce426398e01ac370a29dcc",
      "8b7f64574bb7412f9cef3a4b55f1692d",
      "0c0264d8396447e89442fcef992cc4af",
      "93d44555d89043b49a3858eb5386dbbc",
      "db85b87b84e34604be2af2a049d1bba2",
      "7fa2ff4d57ba4bf38a99232d81deae60",
      "e956b99f5e50409d9906e89fb5e7e5c5",
      "98750e48bb2046bba4765b93613d4806",
      "dbb3e63c6f734c9e9a98cc0452ec850e",
      "2e1a43e6ec4c4aad936882613a28a932",
      "eab3c380ceb640c8b087aeb160855a36",
      "dbabfa6db9b84e7ebf9b99670551bc0d",
      "aef873bcade541969f90b8e8fe6eef41",
      "5d0f3e6ef7334805a6c673a8ffd09f03",
      "3878eb096b0a4ec7b316a6a5684721f4",
      "54d7304592b94e108444ecd44dcc8605",
      "40a8277c15844c61b769ff4077d23b51",
      "484e0b654ecb4654b8019eb2b7c06585",
      "b86f09fb04ce40b6a81d44d778c1e938",
      "444c42a60bf44725a8e4259731fb9d4f",
      "e17ce0e342f24219a736c9090597faa9"
     ]
    },
    "id": "figrnxbWtmLv",
    "outputId": "21c44620-6daa-47da-a2f2-f68374521adc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea830d83cfd46aba380279b9cec66e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 13:49:07 INFO: Downloaded file to C:\\Users\\brike\\stanza_resources\\resources.json\n",
      "2025-06-11 13:49:07 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-06-11 13:49:09 INFO: File exists: C:\\Users\\brike\\stanza_resources\\en\\default.zip\n",
      "2025-06-11 13:49:12 INFO: Finished downloading models and saved to C:\\Users\\brike\\stanza_resources\n",
      "2025-06-11 13:49:12 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d93a6b02cab4fe98053e581a3722c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 13:49:13 INFO: Downloaded file to C:\\Users\\brike\\stanza_resources\\resources.json\n",
      "2025-06-11 13:49:13 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-06-11 13:49:13 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| pos       | combined_charlm           |\n",
      "| lemma     | combined_nocharlm         |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2025-06-11 13:49:13 WARNING: GPU requested, but is not available!\n",
      "2025-06-11 13:49:13 INFO: Using device: cpu\n",
      "2025-06-11 13:49:13 INFO: Loading: tokenize\n",
      "2025-06-11 13:49:13 INFO: Loading: mwt\n",
      "2025-06-11 13:49:13 INFO: Loading: pos\n",
      "2025-06-11 13:49:16 INFO: Loading: lemma\n",
      "2025-06-11 13:49:17 INFO: Loading: ner\n",
      "2025-06-11 13:49:21 INFO: Done loading processors!\n",
      "No sentence-transformers model found with name eventdata-utd/ConfliBERT-scr-uncased. Creating a new one with mean pooling.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at eventdata-utd/ConfliBERT-scr-uncased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data load...\n",
      "Data loaded.\n",
      "Starting training loop...\n",
      "Epoch 1 started\n",
      "Epoch 1 completed\n",
      "Epoch 2 started\n",
      "Epoch 2 completed\n",
      "Epoch 3 started\n",
      "Epoch 3 completed\n",
      "Epoch 4 started\n",
      "Epoch 4 completed\n",
      "Epoch 5 started\n",
      "Epoch 5 completed\n",
      "Epoch 6 started\n",
      "Epoch 6 completed\n",
      "Epoch 7 started\n",
      "Epoch 7 completed\n",
      "Epoch 8 started\n",
      "Epoch 8 completed\n",
      "Epoch 9 started\n",
      "Epoch 9 completed\n",
      "Epoch 10 started\n",
      "Epoch 10 completed\n",
      "✅ Ultra CASS+ Evaluation Completed\n",
      "→ Outputs saved to: C:/Users/brike/CASS/DEEP\\output_cass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brike\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# === 🧠 Ultra CASS+ Evaluation Script (Final Enhanced Version with Robust Filtering and Acronym Fixes) ===\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import stanza\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from unidecode import unidecode\n",
    "import langid\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "# === Constants ===\n",
    "ORTHO_PENALTY_DIACRITIC = 0.10\n",
    "ORTHO_PENALTY_ORTHO = 0.15\n",
    "ORTHO_PENALTY_CHAR_NOISE = 0.20\n",
    "STATIC_THRESHOLD = 0.62\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# === Setup ===\n",
    "DATA_PATH = \"C:/Users/brike/CASS/DEEP\"\n",
    "OUTPUT_PATH = os.path.join(DATA_PATH, \"output_cass\")\n",
    "STATS_PATH = os.path.join(OUTPUT_PATH, \"stats\")\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(STATS_PATH, exist_ok=True)\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma,pos,ner', use_gpu=True)\n",
    "\n",
    "model = SentenceTransformer(\"eventdata-utd/ConfliBERT-scr-uncased\")\n",
    "\n",
    "# === Fix encoding corruption from legacy character sets ===\n",
    "def fix_encoding(text):\n",
    "    try:\n",
    "        return text.encode(\"latin1\").decode(\"utf8\")\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# === Load CSV files with fallback for encoding issues ===\n",
    "def safe_read_csv(path):\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding=\"latin1\")\n",
    "            for col in df.columns:\n",
    "                df[col] = df[col].apply(lambda x: fix_encoding(str(x)) if isinstance(x, str) else x)\n",
    "            return df\n",
    "        except Exception:\n",
    "            return pd.read_csv(path, encoding=\"utf-8\", errors=\"replace\")\n",
    "\n",
    "\n",
    "df_pairs = safe_read_csv(os.path.join(DATA_PATH, \"sentence_pairs.csv\"))\n",
    "lexicon_df = safe_read_csv(os.path.join(DATA_PATH, \"conflict_lexicon.csv\"))\n",
    "acronym_map = {}\n",
    "acro_path = os.path.join(DATA_PATH, \"acronyms_expansions.csv\")\n",
    "if os.path.exists(acro_path):\n",
    "    acro_df = pd.read_csv(acro_path)\n",
    "    for _, row in acro_df.iterrows():\n",
    "        acronym_map[str(row['lost_rare_lemma']).lower()] = str(row['representative_translation_unit']).lower()\n",
    "\n",
    "lexicon_df['term'] = lexicon_df['term'].apply(fix_encoding)\n",
    "lexicon = {t.lower() for t in lexicon_df['term'] if len(t) > 2 or t.isupper()}\n",
    "\n",
    "# === Helper Functions ===\n",
    "def extract_abbreviation_pairs(text):\n",
    "    pairs = {}\n",
    "    matches = re.findall(r'\\(([A-Z]{2,})\\)', text)\n",
    "    for match in matches:\n",
    "        pattern = r'([\\w\\s\\-]+)\\s+\\(' + match + r'\\)'\n",
    "        full_match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if full_match:\n",
    "            pairs[match.lower()] = full_match.group(1).strip()\n",
    "    return pairs\n",
    "\n",
    "def matches_acronym_expansion(acronym, mt_text):\n",
    "    words = [w for w in mt_text.split() if w and w[0].isupper()]\n",
    "    pattern = ''.join(w[0].upper() for w in words[:len(acronym)])\n",
    "    return pattern == acronym.upper()\n",
    "\n",
    "def is_diacritic_variant(term, candidate):\n",
    "    return term != candidate and unidecode(term) == unidecode(candidate) and any(ord(c) > 127 for c in term + candidate)\n",
    "\n",
    "def is_char_noise_match(term, candidate):\n",
    "    return SequenceMatcher(None, term.lower(), candidate.lower()).ratio() >= 0.85\n",
    "\n",
    "def is_weird_token(token):\n",
    "    return any(ord(c) > 126 for c in token) or any(c in token for c in ['\\u2019', '\\u201c', '\\u201d', '\\u00a0'])\n",
    "\n",
    "def get_stanza_features(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas, pos_tags, ents = set(), {}, set()\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            lemmas.add(word.lemma.lower())\n",
    "            pos_tags[word.text.lower()] = word.upos\n",
    "        for ent in sent.ents:\n",
    "            ents.add(ent.text.lower())\n",
    "    return lemmas, pos_tags, ents\n",
    "\n",
    "# === Evaluation ===\n",
    "records = []\n",
    "for _, row in df_pairs.iterrows():\n",
    "    sid, original, mt = str(row['id']), str(row['Original_EN']), str(row['MT_EN'])\n",
    "    abbrev_map = extract_abbreviation_pairs(original)\n",
    "    lang, _ = langid.classify(mt)\n",
    "    if lang != 'en':\n",
    "        continue\n",
    "\n",
    "    orig_lemmas, orig_pos, orig_ents = get_stanza_features(original)\n",
    "    mt_lemmas, mt_pos, mt_ents = get_stanza_features(mt)\n",
    "\n",
    "    for term in lexicon:\n",
    "        if term in orig_lemmas and term not in mt_lemmas:\n",
    "            ner_flag = \"NER_Drop\" if term in orig_ents and term not in mt_ents else \"No\"\n",
    "            pos_flag = \"POS_Mismatch\" if term in orig_pos and term in mt_pos and orig_pos[term] != mt_pos[term] else \"No\"\n",
    "\n",
    "            if term in acronym_map and acronym_map[term] in mt.lower():\n",
    "                records.append({\"Sentence_ID\": sid, \"Original_EN\": original, \"MT_EN\": mt,\n",
    "                                \"Missing_Term\": term, \"Best_Candidate\": acronym_map[term],\n",
    "                                \"Similarity_Score\": 1.0, \"Conflict_Match\": 1, \"CASS_Score\": 1.0,\n",
    "                                \"Orthographic_Diff\": \"AcronymCSV\", \"CharNoise_Flag\": \"No\",\n",
    "                                \"Abbreviation_Match\": \"CSV\", \"NER_Match\": ner_flag, \"POS_Match\": pos_flag,\n",
    "                                \"Explanation_GMM\": \"AcronymCSV\", \"Explanation_Static\": \"AcronymCSV\"})\n",
    "                continue\n",
    "\n",
    "            if term.isupper() and matches_acronym_expansion(term, mt):\n",
    "                records.append({\"Sentence_ID\": sid, \"Original_EN\": original, \"MT_EN\": mt,\n",
    "                                \"Missing_Term\": term, \"Best_Candidate\": \"[expanded]\",\n",
    "                                \"Similarity_Score\": 1.0, \"Conflict_Match\": 1, \"CASS_Score\": 1.0,\n",
    "                                \"Orthographic_Diff\": \"AcronymPattern\", \"CharNoise_Flag\": \"No\",\n",
    "                                \"Abbreviation_Match\": \"Pattern\", \"NER_Match\": ner_flag, \"POS_Match\": pos_flag,\n",
    "                                \"Explanation_GMM\": \"AcronymPattern\", \"Explanation_Static\": \"AcronymPattern\"})\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                src_emb = model.encode(term, convert_to_tensor=True)\n",
    "                mt_embs = model.encode(list(mt_lemmas), convert_to_tensor=True)\n",
    "                sims = util.pytorch_cos_sim(src_emb, mt_embs)[0]\n",
    "                best_idx = sims.argmax().item()\n",
    "                best_cand = list(mt_lemmas)[best_idx]\n",
    "                sim = sims[best_idx].item()\n",
    "                match = 1 if best_cand in lexicon else 0\n",
    "\n",
    "                if best_cand == term:\n",
    "                    ortho_diff, penalty, char_noise = \"No\", 0, \"No\"\n",
    "                elif is_diacritic_variant(term, best_cand):\n",
    "                    ortho_diff, penalty, char_noise = \"Diacritic\", ORTHO_PENALTY_DIACRITIC, \"No\"\n",
    "                elif unidecode(term) == unidecode(best_cand):\n",
    "                    ortho_diff, penalty, char_noise = \"Orthographic\", ORTHO_PENALTY_ORTHO, \"No\"\n",
    "                elif is_char_noise_match(term, best_cand):\n",
    "                    ortho_diff, penalty, char_noise = \"CharNoise\", ORTHO_PENALTY_CHAR_NOISE, \"Yes\"\n",
    "                elif is_weird_token(best_cand):\n",
    "                    ortho_diff, penalty, char_noise = \"WeirdChar\", ORTHO_PENALTY_CHAR_NOISE, \"Yes\"\n",
    "                else:\n",
    "                    ortho_diff, penalty, char_noise = \"No\", 0, \"No\"\n",
    "\n",
    "                cass = round(max(0, 0.7 * sim + 0.3 * match - penalty), 4)\n",
    "\n",
    "                records.append({\"Sentence_ID\": sid, \"Original_EN\": original, \"MT_EN\": mt,\n",
    "                                \"Missing_Term\": term, \"Best_Candidate\": best_cand,\n",
    "                                \"Similarity_Score\": round(sim, 4), \"Conflict_Match\": match, \"CASS_Score\": cass,\n",
    "                                \"Orthographic_Diff\": ortho_diff, \"CharNoise_Flag\": char_noise,\n",
    "                                \"Abbreviation_Match\": \"No\", \"NER_Match\": ner_flag, \"POS_Match\": pos_flag,\n",
    "                                \"Explanation_GMM\": \"\", \"Explanation_Static\": \"\"})\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error in sentence {sid}: {e}\")\n",
    "print(\"Starting data load...\")\n",
    "# Your data loading code here\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "print(\"Starting training loop...\")\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch+1} started\")\n",
    "    # Your training code here\n",
    "    print(f\"Epoch {epoch+1} completed\")\n",
    "\n",
    "# === Scoring ===\n",
    "df = pd.DataFrame(records)\n",
    "if df[\"CASS_Score\"].nunique() > 1:\n",
    "    gmm = GaussianMixture(n_components=2).fit(df[[\"CASS_Score\"]])\n",
    "    thresh_gmm = gmm.means_.mean()\n",
    "else:\n",
    "    thresh_gmm = STATIC_THRESHOLD\n",
    "\n",
    "def explain(row, threshold):\n",
    "    if pd.isna(row[\"CASS_Score\"]):\n",
    "        return \"Error\"\n",
    "    f = []\n",
    "    if row[\"Similarity_Score\"] < 0.4:\n",
    "        f.append(\"LowSim\")\n",
    "    if row[\"Conflict_Match\"] == 0:\n",
    "        f.append(\"NoLexMatch\")\n",
    "    if row[\"Orthographic_Diff\"] == \"Diacritic\":\n",
    "        f.append(\"DiacriticShift\")\n",
    "    elif row[\"Orthographic_Diff\"] == \"Orthographic\":\n",
    "        f.append(\"OrthoDivergence\")\n",
    "    elif row[\"Orthographic_Diff\"] == \"CharNoise\":\n",
    "        f.append(\"CharNoiseMatch\")\n",
    "    elif row[\"Orthographic_Diff\"] == \"WeirdChar\":\n",
    "        f.append(\"WeirdChar\")\n",
    "    if row[\"Abbreviation_Match\"] != \"No\":\n",
    "        f.append(f\"Abbr:{row['Abbreviation_Match']}\")\n",
    "    if row[\"NER_Match\"] == \"NER_Drop\":\n",
    "        f.append(\"NER_Drop\")\n",
    "    if row[\"POS_Match\"] == \"POS_Mismatch\":\n",
    "        f.append(\"POS_Mismatch\")\n",
    "    return f\"{'|'.join(f) if f else 'ExactMatch'} | CASS={row['CASS_Score']:.2f} ≥ {threshold:.2f}\"\n",
    "\n",
    "df[\"Classification_GMM\"] = df[\"CASS_Score\"].apply(lambda x: \"Acceptable\" if x >= thresh_gmm else \"Divergence\")\n",
    "df[\"Classification_Static\"] = df[\"CASS_Score\"].apply(lambda x: \"Acceptable\" if x >= STATIC_THRESHOLD else \"Divergence\")\n",
    "df[\"Explanation_GMM\"] = df.apply(lambda r: explain(r, thresh_gmm), axis=1)\n",
    "df[\"Explanation_Static\"] = df.apply(lambda r: explain(r, STATIC_THRESHOLD), axis=1)\n",
    "\n",
    "# === Save Outputs ===\n",
    "df.to_csv(os.path.join(OUTPUT_PATH, \"mt_eval_CASS.csv\"), index=False)\n",
    "df[[\"Sentence_ID\", \"Original_EN\", \"MT_EN\", \"Missing_Term\", \"Best_Candidate\", \"Orthographic_Diff\", \"CharNoise_Flag\", \"Abbreviation_Match\", \"NER_Match\", \"POS_Match\"]].to_csv(\n",
    "    os.path.join(OUTPUT_PATH, \"missing_terms.csv\"), index=False)\n",
    "\n",
    "summary = {\n",
    "    \"Total Terms\": len(df),\n",
    "    \"Accepted (GMM)\": (df[\"Classification_GMM\"] == \"Acceptable\").sum(),\n",
    "    \"Divergence (GMM)\": (df[\"Classification_GMM\"] == \"Divergence\").sum(),\n",
    "    \"Accepted (Static)\": (df[\"Classification_Static\"] == \"Acceptable\").sum(),\n",
    "    \"Divergence (Static)\": (df[\"Classification_Static\"] == \"Divergence\").sum(),\n",
    "    \"Orthographic Errors\": (df[\"Orthographic_Diff\"] == \"Orthographic\").sum(),\n",
    "    \"Diacritic Errors\": (df[\"Orthographic_Diff\"] == \"Diacritic\").sum(),\n",
    "    \"CharNoise Matches\": (df[\"Orthographic_Diff\"] == \"CharNoise\").sum(),\n",
    "    \"WeirdChar Issues\": (df[\"Orthographic_Diff\"] == \"WeirdChar\").sum(),\n",
    "    \"Abbreviation Matches\": (df[\"Abbreviation_Match\"] != \"No\").sum(),\n",
    "    \"NER Drops\": (df[\"NER_Match\"] == \"NER_Drop\").sum(),\n",
    "    \"POS Mismatches\": (df[\"POS_Match\"] == \"POS_Mismatch\").sum(),\n",
    "    \"Threshold GMM\": round(thresh_gmm, 3),\n",
    "    \"Threshold Static\": STATIC_THRESHOLD\n",
    "}\n",
    "\n",
    "pd.DataFrame([summary]).to_csv(os.path.join(STATS_PATH, \"cass_summary_stats.csv\"), index=False)\n",
    "\n",
    "print(\"✅ Ultra CASS+ Evaluation Completed\")\n",
    "print(f\"→ Outputs saved to: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-M8BKeKSJtR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okGmE8hi69w5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6zXvVh46937"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PTVIYexS5Cf",
    "outputId": "d33357fa-0cd3-450f-fd1f-c22e3ca1853d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Using classification threshold: 0.7310\n",
      "\n",
      "✅ Enhanced CASS Aggregation Completed\n",
      "→ Classified 331 term instances\n",
      "→ Analyzed 273 sentences\n",
      "→ Divergence rate: 91.2%\n",
      "→ Orthographic errors: 0 terms\n",
      "→ Diacritic errors: 38 terms\n",
      "→ Output files saved to C:/Users/brike/CASS/DEEP\\output_cass\n"
     ]
    }
   ],
   "source": [
    "# === 2️⃣ Enhanced Sentence-Level Aggregation for CASS Results (with Diacritic/Ortho Analytics) ===\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "DATA_PATH = \"C:/Users/brike/CASS/DEEP\"\n",
    "OUTPUT_PATH = os.path.join(DATA_PATH, \"output_cass\")\n",
    "TERM_FREQ_FILE = os.path.join(OUTPUT_PATH, \"cass_missing_term_frequencies.csv\")\n",
    "SENTENCE_SUMMARY_FILE = os.path.join(OUTPUT_PATH, \"cass_sentence_level_summary.csv\")\n",
    "EXPANDED_SUMMARY_FILE = os.path.join(OUTPUT_PATH, \"cass_expanded_term_summary.csv\")\n",
    "DIVERGENCE_REPORT = os.path.join(OUTPUT_PATH, \"cass_divergence_breakdown.csv\")\n",
    "\n",
    "# === Threshold Logic ===\n",
    "FALLBACK_THRESHOLD = 0.62\n",
    "stats_path = os.path.join(OUTPUT_PATH, \"stats/cass_summary_stats.csv\")\n",
    "if os.path.exists(stats_path):\n",
    "    stats_df = pd.read_csv(stats_path)\n",
    "    if \"Threshold GMM\" in stats_df.columns:\n",
    "        CLASSIFICATION_THRESHOLD = stats_df[\"Threshold GMM\"].iloc[0]\n",
    "    else:\n",
    "        CLASSIFICATION_THRESHOLD = FALLBACK_THRESHOLD\n",
    "else:\n",
    "    CLASSIFICATION_THRESHOLD = FALLBACK_THRESHOLD\n",
    "\n",
    "print(f\"🔧 Using classification threshold: {CLASSIFICATION_THRESHOLD:.4f}\")\n",
    "\n",
    "# === Load Data ===\n",
    "df = pd.read_csv(os.path.join(OUTPUT_PATH, \"mt_eval_CASS.csv\"))\n",
    "\n",
    "# === Enhanced Classification ===\n",
    "if 'Classification_Static' in df.columns:\n",
    "    df['Missing_Term_Classification'] = df['Classification_Static']\n",
    "else:\n",
    "    df['Missing_Term_Classification'] = np.where(\n",
    "        df['CASS_Score'] >= CLASSIFICATION_THRESHOLD,\n",
    "        'Acceptable',\n",
    "        'Divergence'\n",
    "    )\n",
    "\n",
    "# === Term Frequency (Excluding Non-English) ===\n",
    "valid_terms = df[df['Missing_Term'] != \"—\"]\n",
    "term_freq = valid_terms['Missing_Term'].value_counts().reset_index()\n",
    "term_freq.columns = ['Missing_Term', 'Frequency']\n",
    "term_freq = term_freq.merge(\n",
    "    valid_terms.groupby('Missing_Term')['CASS_Score'].mean().reset_index(),\n",
    "    on='Missing_Term',\n",
    "    how='left'\n",
    ").rename(columns={'CASS_Score': 'Avg_CASS'})\n",
    "term_freq['Divergence_Rate'] = valid_terms.groupby('Missing_Term')['Missing_Term_Classification'].apply(\n",
    "    lambda x: (x == 'Divergence').mean()\n",
    ").reset_index()['Missing_Term_Classification']\n",
    "term_freq.to_csv(TERM_FREQ_FILE, index=False)\n",
    "\n",
    "# === Enhanced Explanation Flags ===\n",
    "def generate_explanation(row):\n",
    "    reasons = []\n",
    "    if 'NonEnglish' in str(row.get('Explanation_Static', '')):\n",
    "        return \"NonEnglish\"\n",
    "    sim = row.get('Similarity_Score', 0)\n",
    "    if sim < 0.4:\n",
    "        reasons.append(f\"LowSim({sim:.2f})\")\n",
    "    if row.get('Conflict_Match', 0) == 0:\n",
    "        reasons.append(\"NoLexMatch\")\n",
    "    ortho = row.get('Orthographic_Diff', 'No')\n",
    "    if ortho == \"Diacritic\":\n",
    "        reasons.append(\"DiacriticShift\")\n",
    "    elif ortho == \"Yes\":\n",
    "        reasons.append(\"OrthoDivergence\")\n",
    "    cass = row.get('CASS_Score', 0)\n",
    "    if cass < CLASSIFICATION_THRESHOLD:\n",
    "        reasons.append(f\"Threshold({cass:.2f}<{CLASSIFICATION_THRESHOLD:.2f})\")\n",
    "    return \"|\".join(reasons) if reasons else \"AllGood\"\n",
    "\n",
    "df['Explanation_Flag'] = df.apply(generate_explanation, axis=1)\n",
    "\n",
    "# === Save Term-Level Expanded Output ===\n",
    "term_cols = [\n",
    "    'Sentence_ID', 'Original_EN', 'MT_EN', 'Missing_Term',\n",
    "    'Best_Candidate', 'CASS_Score', 'Similarity_Score',\n",
    "    'Conflict_Match', 'Orthographic_Diff', 'Missing_Term_Classification',\n",
    "    'Explanation_Flag'\n",
    "]\n",
    "if 'MT_Lang' in df.columns:\n",
    "    term_cols.append('MT_Lang')\n",
    "df[term_cols].to_csv(EXPANDED_SUMMARY_FILE, index=False)\n",
    "\n",
    "# === Robust Sentence-Level Aggregation (with Ortho/Diacritic Error Breakdown) ===\n",
    "df['Is_Divergence'] = df['Missing_Term_Classification'] == 'Divergence'\n",
    "df['Is_Ortho_Error'] = df['Orthographic_Diff'] == 'Yes'\n",
    "df['Is_Diacritic_Error'] = df['Orthographic_Diff'] == 'Diacritic'\n",
    "df['Is_Lex_Mismatch'] = df['Conflict_Match'] == 0\n",
    "df['Is_Low_Similarity'] = df['Similarity_Score'] < 0.4\n",
    "\n",
    "agg_config = {\n",
    "    'Original_EN': 'first',\n",
    "    'MT_EN': 'first',\n",
    "    'CASS_Score': ['count', 'min', 'max', 'mean'],\n",
    "    'Is_Divergence': 'sum',\n",
    "    'Is_Ortho_Error': 'sum',\n",
    "    'Is_Diacritic_Error': 'sum',\n",
    "    'Is_Lex_Mismatch': 'sum',\n",
    "    'Is_Low_Similarity': 'sum',\n",
    "    'Missing_Term_Classification': lambda x: '|'.join(x),\n",
    "    'Explanation_Flag': lambda x: '||'.join(x)\n",
    "}\n",
    "if 'MT_Lang' in df.columns:\n",
    "    agg_config['MT_Lang'] = 'first'\n",
    "\n",
    "sentence_summary = df.groupby('Sentence_ID').agg(agg_config).reset_index()\n",
    "sentence_summary.columns = [\n",
    "    'Sentence_ID',\n",
    "    'Original_EN',\n",
    "    'MT_EN',\n",
    "    'Num_Missing_Terms',\n",
    "    'Min_CASS',\n",
    "    'Max_CASS',\n",
    "    'Mean_CASS',\n",
    "    'Num_Divergent_Terms',\n",
    "    'Num_Orthographic_Errors',\n",
    "    'Num_Diacritic_Errors',\n",
    "    'Num_Lexicon_Mismatches',\n",
    "    'Num_Low_Similarity',\n",
    "    'Term_Classifications',\n",
    "    'Term_Explanation_Flags'\n",
    "] + (['MT_Lang'] if 'MT_Lang' in df.columns else [])\n",
    "\n",
    "# Sentence-level labels\n",
    "sentence_summary['Sentence_Classification_Mean'] = np.where(\n",
    "    sentence_summary['Mean_CASS'] >= CLASSIFICATION_THRESHOLD,\n",
    "    'Acceptable',\n",
    "    'Divergence'\n",
    ")\n",
    "sentence_summary['Sentence_Classification_Strict'] = np.where(\n",
    "    sentence_summary['Num_Divergent_Terms'] > 0,\n",
    "    'Divergence',\n",
    "    'Acceptable'\n",
    ")\n",
    "sentence_summary['Divergence_Severity'] = np.where(\n",
    "    sentence_summary['Mean_CASS'] < CLASSIFICATION_THRESHOLD - 0.2,\n",
    "    'High',\n",
    "    np.where(sentence_summary['Mean_CASS'] < CLASSIFICATION_THRESHOLD, 'Medium', 'None')\n",
    ")\n",
    "\n",
    "sentence_summary.to_csv(SENTENCE_SUMMARY_FILE, index=False)\n",
    "\n",
    "# === Divergence Analytics Report (With Explicit Column Names) ===\n",
    "divergence_terms = df[df['Missing_Term_Classification'] == 'Divergence'][[\n",
    "    'Sentence_ID', 'Original_EN', 'MT_EN', 'Missing_Term',\n",
    "    'Best_Candidate', 'CASS_Score', 'Similarity_Score', 'Conflict_Match',\n",
    "    'Orthographic_Diff', 'Explanation_Flag'\n",
    "] + (['MT_Lang'] if 'MT_Lang' in df.columns else [])]\n",
    "\n",
    "divergence_terms.to_csv(DIVERGENCE_REPORT, index=False)\n",
    "\n",
    "# === Final Output ===\n",
    "print(\"\\n✅ Enhanced CASS Aggregation Completed\")\n",
    "print(f\"→ Classified {len(df)} term instances\")\n",
    "print(f\"→ Analyzed {len(sentence_summary)} sentences\")\n",
    "print(f\"→ Divergence rate: {sentence_summary['Num_Divergent_Terms'].sum()/len(df):.1%}\")\n",
    "print(f\"→ Orthographic errors: {df['Orthographic_Diff'].eq('Yes').sum()} terms\")\n",
    "print(f\"→ Diacritic errors: {df['Orthographic_Diff'].eq('Diacritic').sum()} terms\")\n",
    "print(f\"→ Output files saved to {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6v8n6ItG1_kX",
    "outputId": "ef4d909d-693c-4305-c06f-33f3af46ef41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All CASS visualizations and summary report saved to:\n",
      "→ C:/Users/brike/CASS/DEEP\\output_cass\\stats\n"
     ]
    }
   ],
   "source": [
    "# === 3️⃣ Enhanced CASS Visualization & Summary Script (Palette/Hue Fix) ===\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# === Paths ===\n",
    "DATA_PATH = \"C:/Users/brike/CASS/DEEP\"\n",
    "OUTPUT_PATH = os.path.join(DATA_PATH, \"output_cass\")\n",
    "STATS_PATH = os.path.join(OUTPUT_PATH, \"stats\")\n",
    "os.makedirs(STATS_PATH, exist_ok=True)\n",
    "\n",
    "# === Load Data ===\n",
    "df_terms = pd.read_csv(os.path.join(OUTPUT_PATH, \"cass_expanded_term_summary.csv\"))\n",
    "df_sentences = pd.read_csv(os.path.join(OUTPUT_PATH, \"cass_sentence_level_summary.csv\"))\n",
    "term_freq = pd.read_csv(os.path.join(OUTPUT_PATH, \"cass_missing_term_frequencies.csv\"))\n",
    "\n",
    "# === Term-Level CASS Score Distribution ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_terms['CASS_Score'], bins=20, kde=True, color=\"skyblue\")\n",
    "plt.axvline(0.62, color=\"red\", linestyle=\"--\", label=\"Static Threshold = 0.62\")\n",
    "plt.title(\"Term-Level CASS Score Distribution\")\n",
    "plt.xlabel(\"CASS Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(STATS_PATH, \"term_level_cass_distribution.png\"))\n",
    "plt.close()\n",
    "\n",
    "# === Sentence-Level Mean CASS Distribution ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_sentences['Mean_CASS'], bins=20, kde=True, color=\"orange\")\n",
    "plt.axvline(0.62, color=\"red\", linestyle=\"--\", label=\"Static Threshold = 0.62\")\n",
    "plt.title(\"Sentence-Level Mean CASS Score Distribution\")\n",
    "plt.xlabel(\"Mean CASS Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(STATS_PATH, \"sentence_level_cass_distribution.png\"))\n",
    "plt.close()\n",
    "\n",
    "# === Sentence-Level Classification Bar Chart ===\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.countplot(\n",
    "    data=df_sentences,\n",
    "    x=\"Sentence_Classification_Mean\",\n",
    "    hue=\"Sentence_Classification_Mean\",\n",
    "    palette=\"Set2\",\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Sentence-Level Classification (Acceptable vs Divergence)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(STATS_PATH, \"sentence_level_classification.png\"))\n",
    "plt.close()\n",
    "\n",
    "# === Term-Level Classification Bar Chart ===\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.countplot(\n",
    "    data=df_terms,\n",
    "    x=\"Missing_Term_Classification\",\n",
    "    hue=\"Missing_Term_Classification\",\n",
    "    palette=\"coolwarm\",\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Term-Level Classification (Acceptable vs Divergence)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(STATS_PATH, \"term_level_classification.png\"))\n",
    "plt.close()\n",
    "\n",
    "# === Orthographic/Diacritic Error Frequency Plots ===\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.countplot(\n",
    "    data=df_terms,\n",
    "    x=\"Orthographic_Diff\",\n",
    "    hue=\"Orthographic_Diff\",\n",
    "    order=[\"No\", \"Diacritic\", \"Yes\"],\n",
    "    palette=\"rocket\",\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Orthographic & Diacritic Error Breakdown (Terms)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(STATS_PATH, \"term_level_ortho_diakritik_errors.png\"))\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "error_counts = df_terms[\"Orthographic_Diff\"].value_counts()\n",
    "sns.barplot(\n",
    "    x=error_counts.index,\n",
    "    y=error_counts.values,\n",
    "    hue=error_counts.index,\n",
    "    palette=\"rocket\",\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Term Count by Orthographic Error Type\")\n",
    "plt.ylabel(\"Term Count\")\n",
    "plt.xlabel(\"Orthographic Error Type\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(STATS_PATH, \"ortho_error_type_counts.png\"))\n",
    "plt.close()\n",
    "\n",
    "# === Explanation Flag Frequency ===\n",
    "flag_counts = Counter(df_terms['Explanation_Flag'])\n",
    "flag_df = pd.DataFrame(flag_counts.items(), columns=[\"Explanation_Flag\", \"Count\"]).sort_values(\"Count\", ascending=False)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(\n",
    "    data=flag_df,\n",
    "    x=\"Explanation_Flag\",\n",
    "    y=\"Count\",\n",
    "    hue=\"Explanation_Flag\",\n",
    "    palette=\"magma\",\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Explanation Flag Frequency\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(STATS_PATH, \"explanation_flag_frequency.png\"))\n",
    "plt.close()\n",
    "\n",
    "# === Top 20 Most Missing Terms ===\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_terms = term_freq.nlargest(20, 'Frequency')\n",
    "sns.barplot(\n",
    "    data=top_terms,\n",
    "    x=\"Missing_Term\",\n",
    "    y=\"Frequency\",\n",
    "    hue=\"Missing_Term\",\n",
    "    palette=\"viridis\",\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Top 20 Most Frequently Missing Terms\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(STATS_PATH, \"missing_term_frequency_top20.png\"))\n",
    "plt.close()\n",
    "\n",
    "# === Statistics Summary ===\n",
    "summary = {\n",
    "    \"Total Missing Terms\": len(df_terms),\n",
    "    \"Unique Missing Terms\": df_terms['Missing_Term'].nunique(),\n",
    "    \"Total Sentences Evaluated\": len(df_sentences),\n",
    "    \"Average Terms per Sentence\": round(df_terms.shape[0] / df_sentences.shape[0], 2),\n",
    "    \"Term Acceptable %\": round((df_terms['Missing_Term_Classification'] == 'Acceptable').mean() * 100, 2),\n",
    "    \"Sentence Acceptable %\": round((df_sentences['Sentence_Classification_Mean'] == 'Acceptable').mean() * 100, 2),\n",
    "    \"Term-Level Mean CASS\": round(df_terms['CASS_Score'].mean(), 4),\n",
    "    \"Sentence-Level Mean CASS\": round(df_sentences['Mean_CASS'].mean(), 4),\n",
    "    \"Diacritic Error Count\": df_terms[\"Orthographic_Diff\"].eq(\"Diacritic\").sum(),\n",
    "    \"Orthographic Error Count\": df_terms[\"Orthographic_Diff\"].eq(\"Yes\").sum(),\n",
    "    \"No Error Count\": df_terms[\"Orthographic_Diff\"].eq(\"No\").sum(),\n",
    "}\n",
    "pd.DataFrame([summary]).to_csv(os.path.join(STATS_PATH, \"cass_summary_report.csv\"), index=False)\n",
    "\n",
    "print(\"✅ All CASS visualizations and summary report saved to:\")\n",
    "print(f\"→ {STATS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31cMG2zLr-4E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhqbdmXmKTAZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c0264d8396447e89442fcef992cc4af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbb3e63c6f734c9e9a98cc0452ec850e",
      "placeholder": "​",
      "style": "IPY_MODEL_2e1a43e6ec4c4aad936882613a28a932",
      "value": " 428k/? [00:00&lt;00:00, 30.3MB/s]"
     }
    },
    "24e4ca6f90ce426398e01ac370a29dcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db85b87b84e34604be2af2a049d1bba2",
      "placeholder": "​",
      "style": "IPY_MODEL_7fa2ff4d57ba4bf38a99232d81deae60",
      "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
     }
    },
    "2e1a43e6ec4c4aad936882613a28a932": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3878eb096b0a4ec7b316a6a5684721f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40a8277c15844c61b769ff4077d23b51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "444c42a60bf44725a8e4259731fb9d4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "484e0b654ecb4654b8019eb2b7c06585": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54d7304592b94e108444ecd44dcc8605": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d0f3e6ef7334805a6c673a8ffd09f03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_444c42a60bf44725a8e4259731fb9d4f",
      "placeholder": "​",
      "style": "IPY_MODEL_e17ce0e342f24219a736c9090597faa9",
      "value": " 428k/? [00:00&lt;00:00, 17.7MB/s]"
     }
    },
    "7fa2ff4d57ba4bf38a99232d81deae60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b7f64574bb7412f9cef3a4b55f1692d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e956b99f5e50409d9906e89fb5e7e5c5",
      "max": 53069,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_98750e48bb2046bba4765b93613d4806",
      "value": 53069
     }
    },
    "93d44555d89043b49a3858eb5386dbbc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98750e48bb2046bba4765b93613d4806": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aef873bcade541969f90b8e8fe6eef41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_484e0b654ecb4654b8019eb2b7c06585",
      "max": 53069,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b86f09fb04ce40b6a81d44d778c1e938",
      "value": 53069
     }
    },
    "b86f09fb04ce40b6a81d44d778c1e938": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "db85b87b84e34604be2af2a049d1bba2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbabfa6db9b84e7ebf9b99670551bc0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54d7304592b94e108444ecd44dcc8605",
      "placeholder": "​",
      "style": "IPY_MODEL_40a8277c15844c61b769ff4077d23b51",
      "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
     }
    },
    "dbb3e63c6f734c9e9a98cc0452ec850e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e17ce0e342f24219a736c9090597faa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e3de94d87ea24c1696136b571abc9c11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_24e4ca6f90ce426398e01ac370a29dcc",
       "IPY_MODEL_8b7f64574bb7412f9cef3a4b55f1692d",
       "IPY_MODEL_0c0264d8396447e89442fcef992cc4af"
      ],
      "layout": "IPY_MODEL_93d44555d89043b49a3858eb5386dbbc"
     }
    },
    "e956b99f5e50409d9906e89fb5e7e5c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eab3c380ceb640c8b087aeb160855a36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dbabfa6db9b84e7ebf9b99670551bc0d",
       "IPY_MODEL_aef873bcade541969f90b8e8fe6eef41",
       "IPY_MODEL_5d0f3e6ef7334805a6c673a8ffd09f03"
      ],
      "layout": "IPY_MODEL_3878eb096b0a4ec7b316a6a5684721f4"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
