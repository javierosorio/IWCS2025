{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeM324jitXeI",
        "outputId": "abaa2ca0-716b-4756-eaa9-73af6212d19d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-lg==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.1\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from langid) (1.26.4)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941171 sha256=bee5e5b670f321ec847da9f8f8706404485f3f559c4c96ed0fffe0009cbc5fed\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/6a/b6/b7eb43a6ad55b139c15c5daa29f3707659cfa6944d3c696f5b\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n"
          ]
        }
      ],
      "source": [
        "# Core NLP and ML libraries\n",
        "!pip install -q pandas numpy tqdm scikit-learn matplotlib seaborn fpdf\n",
        "\n",
        "# Stanza for tokenization & lemmatization\n",
        "!pip install -q stanza\n",
        "\n",
        "# Sentence-Transformers (for ConfliBERT)\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "# HuggingFace Transformers for tokenizer compatibility\n",
        "!pip install -q transformers\n",
        "\n",
        "# spaCy for POS tagging (optional, but included if needed later)\n",
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "# NLTK for WordNet and lemmatization\n",
        "!pip install -q nltk\n",
        "\n",
        "!pip install -q unidecode\n",
        "#!pip install unidecode\n",
        "\n",
        "!pip install langid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9-3T6b6ul3-",
        "outputId": "ebac3447-25d8-4988-8012-d6a47fc61458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e3de94d87ea24c1696136b571abc9c11",
            "24e4ca6f90ce426398e01ac370a29dcc",
            "8b7f64574bb7412f9cef3a4b55f1692d",
            "0c0264d8396447e89442fcef992cc4af",
            "93d44555d89043b49a3858eb5386dbbc",
            "db85b87b84e34604be2af2a049d1bba2",
            "7fa2ff4d57ba4bf38a99232d81deae60",
            "e956b99f5e50409d9906e89fb5e7e5c5",
            "98750e48bb2046bba4765b93613d4806",
            "dbb3e63c6f734c9e9a98cc0452ec850e",
            "2e1a43e6ec4c4aad936882613a28a932",
            "eab3c380ceb640c8b087aeb160855a36",
            "dbabfa6db9b84e7ebf9b99670551bc0d",
            "aef873bcade541969f90b8e8fe6eef41",
            "5d0f3e6ef7334805a6c673a8ffd09f03",
            "3878eb096b0a4ec7b316a6a5684721f4",
            "54d7304592b94e108444ecd44dcc8605",
            "40a8277c15844c61b769ff4077d23b51",
            "484e0b654ecb4654b8019eb2b7c06585",
            "b86f09fb04ce40b6a81d44d778c1e938",
            "444c42a60bf44725a8e4259731fb9d4f",
            "e17ce0e342f24219a736c9090597faa9"
          ]
        },
        "id": "figrnxbWtmLv",
        "outputId": "21c44620-6daa-47da-a2f2-f68374521adc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  â€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3de94d87ea24c1696136b571abc9c11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: en (English) ...\n",
            "INFO:stanza:File exists: /root/stanza_resources/en/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  â€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eab3c380ceb640c8b087aeb160855a36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "WARNING:stanza:Language en package default expects mwt, which has been added\n",
            "INFO:stanza:Loading these models for language: en (English):\n",
            "=========================================\n",
            "| Processor | Package                   |\n",
            "-----------------------------------------\n",
            "| tokenize  | combined                  |\n",
            "| mwt       | combined                  |\n",
            "| pos       | combined_charlm           |\n",
            "| lemma     | combined_nocharlm         |\n",
            "| ner       | ontonotes-ww-multi_charlm |\n",
            "=========================================\n",
            "\n",
            "WARNING:stanza:GPU requested, but is not available!\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name eventdata-utd/ConfliBERT-scr-uncased. Creating a new one with mean pooling.\n",
            "Some weights of BertModel were not initialized from the model checkpoint at eventdata-utd/ConfliBERT-scr-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data load...\n",
            "Data loaded.\n",
            "Starting training loop...\n",
            "Epoch 1 started\n",
            "Epoch 1 completed\n",
            "Epoch 2 started\n",
            "Epoch 2 completed\n",
            "Epoch 3 started\n",
            "Epoch 3 completed\n",
            "Epoch 4 started\n",
            "Epoch 4 completed\n",
            "Epoch 5 started\n",
            "Epoch 5 completed\n",
            "Epoch 6 started\n",
            "Epoch 6 completed\n",
            "Epoch 7 started\n",
            "Epoch 7 completed\n",
            "Epoch 8 started\n",
            "Epoch 8 completed\n",
            "Epoch 9 started\n",
            "Epoch 9 completed\n",
            "Epoch 10 started\n",
            "Epoch 10 completed\n",
            "âœ… Ultra CASS+ Evaluation Completed\n",
            "â†’ Outputs saved to: /content/drive/MyDrive/Summer/CASS/output_cass\n"
          ]
        }
      ],
      "source": [
        "# === ğŸ§  Ultra CASS+ Evaluation Script (Final Enhanced Version with Robust Filtering and Acronym Fixes) ===\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import stanza\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from unidecode import unidecode\n",
        "import langid\n",
        "from difflib import SequenceMatcher\n",
        "import re\n",
        "\n",
        "# === Constants ===\n",
        "ORTHO_PENALTY_DIACRITIC = 0.10\n",
        "ORTHO_PENALTY_ORTHO = 0.15\n",
        "ORTHO_PENALTY_CHAR_NOISE = 0.20\n",
        "STATIC_THRESHOLD = 0.62\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# === Setup ===\n",
        "DATA_PATH = \"/content/drive/MyDrive/Summer/CASS\"\n",
        "OUTPUT_PATH = os.path.join(DATA_PATH, \"output_cass\")\n",
        "STATS_PATH = os.path.join(OUTPUT_PATH, \"stats\")\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "os.makedirs(STATS_PATH, exist_ok=True)\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stanza.download('en')\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma,pos,ner', use_gpu=True)\n",
        "\n",
        "model = SentenceTransformer(\"eventdata-utd/ConfliBERT-scr-uncased\")\n",
        "\n",
        "# === Fix encoding corruption from legacy character sets ===\n",
        "def fix_encoding(text):\n",
        "    try:\n",
        "        return text.encode(\"latin1\").decode(\"utf8\")\n",
        "    except:\n",
        "        return text\n",
        "\n",
        "# === Load CSV files with fallback for encoding issues ===\n",
        "def safe_read_csv(path):\n",
        "    try:\n",
        "        return pd.read_csv(path, encoding=\"utf-8\")\n",
        "    except UnicodeDecodeError:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=\"latin1\")\n",
        "            for col in df.columns:\n",
        "                df[col] = df[col].apply(lambda x: fix_encoding(str(x)) if isinstance(x, str) else x)\n",
        "            return df\n",
        "        except Exception:\n",
        "            return pd.read_csv(path, encoding=\"utf-8\", errors=\"replace\")\n",
        "\n",
        "\n",
        "df_pairs = safe_read_csv(os.path.join(DATA_PATH, \"sentence_pairs.csv\"))\n",
        "lexicon_df = safe_read_csv(os.path.join(DATA_PATH, \"conflict_lexicon.csv\"))\n",
        "acronym_map = {}\n",
        "acro_path = os.path.join(DATA_PATH, \"acronyms_expansions.csv\")\n",
        "if os.path.exists(acro_path):\n",
        "    acro_df = pd.read_csv(acro_path)\n",
        "    for _, row in acro_df.iterrows():\n",
        "        acronym_map[str(row['lost_rare_lemma']).lower()] = str(row['representative_translation_unit']).lower()\n",
        "\n",
        "lexicon_df['term'] = lexicon_df['term'].apply(fix_encoding)\n",
        "lexicon = {t.lower() for t in lexicon_df['term'] if len(t) > 2 or t.isupper()}\n",
        "\n",
        "# === Helper Functions ===\n",
        "def extract_abbreviation_pairs(text):\n",
        "    pairs = {}\n",
        "    matches = re.findall(r'\\(([A-Z]{2,})\\)', text)\n",
        "    for match in matches:\n",
        "        pattern = r'([\\w\\s\\-]+)\\s+\\(' + match + r'\\)'\n",
        "        full_match = re.search(pattern, text, re.IGNORECASE)\n",
        "        if full_match:\n",
        "            pairs[match.lower()] = full_match.group(1).strip()\n",
        "    return pairs\n",
        "\n",
        "def matches_acronym_expansion(acronym, mt_text):\n",
        "    words = [w for w in mt_text.split() if w and w[0].isupper()]\n",
        "    pattern = ''.join(w[0].upper() for w in words[:len(acronym)])\n",
        "    return pattern == acronym.upper()\n",
        "\n",
        "def is_diacritic_variant(term, candidate):\n",
        "    return term != candidate and unidecode(term) == unidecode(candidate) and any(ord(c) > 127 for c in term + candidate)\n",
        "\n",
        "def is_char_noise_match(term, candidate):\n",
        "    return SequenceMatcher(None, term.lower(), candidate.lower()).ratio() >= 0.85\n",
        "\n",
        "def is_weird_token(token):\n",
        "    return any(ord(c) > 126 for c in token) or any(c in token for c in ['\\u2019', '\\u201c', '\\u201d', '\\u00a0'])\n",
        "\n",
        "def get_stanza_features(text):\n",
        "    doc = nlp(text)\n",
        "    lemmas, pos_tags, ents = set(), {}, set()\n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            lemmas.add(word.lemma.lower())\n",
        "            pos_tags[word.text.lower()] = word.upos\n",
        "        for ent in sent.ents:\n",
        "            ents.add(ent.text.lower())\n",
        "    return lemmas, pos_tags, ents\n",
        "\n",
        "# === Evaluation ===\n",
        "records = []\n",
        "for _, row in df_pairs.iterrows():\n",
        "    sid, original, mt = str(row['id']), str(row['Original_EN']), str(row['MT_EN'])\n",
        "    abbrev_map = extract_abbreviation_pairs(original)\n",
        "    lang, _ = langid.classify(mt)\n",
        "    if lang != 'en':\n",
        "        continue\n",
        "\n",
        "    orig_lemmas, orig_pos, orig_ents = get_stanza_features(original)\n",
        "    mt_lemmas, mt_pos, mt_ents = get_stanza_features(mt)\n",
        "\n",
        "    for term in lexicon:\n",
        "        if term in orig_lemmas and term not in mt_lemmas:\n",
        "            ner_flag = \"NER_Drop\" if term in orig_ents and term not in mt_ents else \"No\"\n",
        "            pos_flag = \"POS_Mismatch\" if term in orig_pos and term in mt_pos and orig_pos[term] != mt_pos[term] else \"No\"\n",
        "\n",
        "            if term in acronym_map and acronym_map[term] in mt.lower():\n",
        "                records.append({\"Sentence_ID\": sid, \"Original_EN\": original, \"MT_EN\": mt,\n",
        "                                \"Missing_Term\": term, \"Best_Candidate\": acronym_map[term],\n",
        "                                \"Similarity_Score\": 1.0, \"Conflict_Match\": 1, \"CASS_Score\": 1.0,\n",
        "                                \"Orthographic_Diff\": \"AcronymCSV\", \"CharNoise_Flag\": \"No\",\n",
        "                                \"Abbreviation_Match\": \"CSV\", \"NER_Match\": ner_flag, \"POS_Match\": pos_flag,\n",
        "                                \"Explanation_GMM\": \"AcronymCSV\", \"Explanation_Static\": \"AcronymCSV\"})\n",
        "                continue\n",
        "\n",
        "            if term.isupper() and matches_acronym_expansion(term, mt):\n",
        "                records.append({\"Sentence_ID\": sid, \"Original_EN\": original, \"MT_EN\": mt,\n",
        "                                \"Missing_Term\": term, \"Best_Candidate\": \"[expanded]\",\n",
        "                                \"Similarity_Score\": 1.0, \"Conflict_Match\": 1, \"CASS_Score\": 1.0,\n",
        "                                \"Orthographic_Diff\": \"AcronymPattern\", \"CharNoise_Flag\": \"No\",\n",
        "                                \"Abbreviation_Match\": \"Pattern\", \"NER_Match\": ner_flag, \"POS_Match\": pos_flag,\n",
        "                                \"Explanation_GMM\": \"AcronymPattern\", \"Explanation_Static\": \"AcronymPattern\"})\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                src_emb = model.encode(term, convert_to_tensor=True)\n",
        "                mt_embs = model.encode(list(mt_lemmas), convert_to_tensor=True)\n",
        "                sims = util.pytorch_cos_sim(src_emb, mt_embs)[0]\n",
        "                best_idx = sims.argmax().item()\n",
        "                best_cand = list(mt_lemmas)[best_idx]\n",
        "                sim = sims[best_idx].item()\n",
        "                match = 1 if best_cand in lexicon else 0\n",
        "\n",
        "                if best_cand == term:\n",
        "                    ortho_diff, penalty, char_noise = \"No\", 0, \"No\"\n",
        "                elif is_diacritic_variant(term, best_cand):\n",
        "                    ortho_diff, penalty, char_noise = \"Diacritic\", ORTHO_PENALTY_DIACRITIC, \"No\"\n",
        "                elif unidecode(term) == unidecode(best_cand):\n",
        "                    ortho_diff, penalty, char_noise = \"Orthographic\", ORTHO_PENALTY_ORTHO, \"No\"\n",
        "                elif is_char_noise_match(term, best_cand):\n",
        "                    ortho_diff, penalty, char_noise = \"CharNoise\", ORTHO_PENALTY_CHAR_NOISE, \"Yes\"\n",
        "                elif is_weird_token(best_cand):\n",
        "                    ortho_diff, penalty, char_noise = \"WeirdChar\", ORTHO_PENALTY_CHAR_NOISE, \"Yes\"\n",
        "                else:\n",
        "                    ortho_diff, penalty, char_noise = \"No\", 0, \"No\"\n",
        "\n",
        "                cass = round(max(0, 0.7 * sim + 0.3 * match - penalty), 4)\n",
        "\n",
        "                records.append({\"Sentence_ID\": sid, \"Original_EN\": original, \"MT_EN\": mt,\n",
        "                                \"Missing_Term\": term, \"Best_Candidate\": best_cand,\n",
        "                                \"Similarity_Score\": round(sim, 4), \"Conflict_Match\": match, \"CASS_Score\": cass,\n",
        "                                \"Orthographic_Diff\": ortho_diff, \"CharNoise_Flag\": char_noise,\n",
        "                                \"Abbreviation_Match\": \"No\", \"NER_Match\": ner_flag, \"POS_Match\": pos_flag,\n",
        "                                \"Explanation_GMM\": \"\", \"Explanation_Static\": \"\"})\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error in sentence {sid}: {e}\")\n",
        "print(\"Starting data load...\")\n",
        "# Your data loading code here\n",
        "print(\"Data loaded.\")\n",
        "\n",
        "print(\"Starting training loop...\")\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    print(f\"Epoch {epoch+1} started\")\n",
        "    # Your training code here\n",
        "    print(f\"Epoch {epoch+1} completed\")\n",
        "\n",
        "# === Scoring ===\n",
        "df = pd.DataFrame(records)\n",
        "if df[\"CASS_Score\"].nunique() > 1:\n",
        "    gmm = GaussianMixture(n_components=2).fit(df[[\"CASS_Score\"]])\n",
        "    thresh_gmm = gmm.means_.mean()\n",
        "else:\n",
        "    thresh_gmm = STATIC_THRESHOLD\n",
        "\n",
        "def explain(row, threshold):\n",
        "    if pd.isna(row[\"CASS_Score\"]):\n",
        "        return \"Error\"\n",
        "    f = []\n",
        "    if row[\"Similarity_Score\"] < 0.4:\n",
        "        f.append(\"LowSim\")\n",
        "    if row[\"Conflict_Match\"] == 0:\n",
        "        f.append(\"NoLexMatch\")\n",
        "    if row[\"Orthographic_Diff\"] == \"Diacritic\":\n",
        "        f.append(\"DiacriticShift\")\n",
        "    elif row[\"Orthographic_Diff\"] == \"Orthographic\":\n",
        "        f.append(\"OrthoDivergence\")\n",
        "    elif row[\"Orthographic_Diff\"] == \"CharNoise\":\n",
        "        f.append(\"CharNoiseMatch\")\n",
        "    elif row[\"Orthographic_Diff\"] == \"WeirdChar\":\n",
        "        f.append(\"WeirdChar\")\n",
        "    if row[\"Abbreviation_Match\"] != \"No\":\n",
        "        f.append(f\"Abbr:{row['Abbreviation_Match']}\")\n",
        "    if row[\"NER_Match\"] == \"NER_Drop\":\n",
        "        f.append(\"NER_Drop\")\n",
        "    if row[\"POS_Match\"] == \"POS_Mismatch\":\n",
        "        f.append(\"POS_Mismatch\")\n",
        "    return f\"{'|'.join(f) if f else 'ExactMatch'} | CASS={row['CASS_Score']:.2f} â‰¥ {threshold:.2f}\"\n",
        "\n",
        "df[\"Classification_GMM\"] = df[\"CASS_Score\"].apply(lambda x: \"Acceptable\" if x >= thresh_gmm else \"Divergence\")\n",
        "df[\"Classification_Static\"] = df[\"CASS_Score\"].apply(lambda x: \"Acceptable\" if x >= STATIC_THRESHOLD else \"Divergence\")\n",
        "df[\"Explanation_GMM\"] = df.apply(lambda r: explain(r, thresh_gmm), axis=1)\n",
        "df[\"Explanation_Static\"] = df.apply(lambda r: explain(r, STATIC_THRESHOLD), axis=1)\n",
        "\n",
        "# === Save Outputs ===\n",
        "df.to_csv(os.path.join(OUTPUT_PATH, \"mt_eval_CASS.csv\"), index=False)\n",
        "df[[\"Sentence_ID\", \"Original_EN\", \"MT_EN\", \"Missing_Term\", \"Best_Candidate\", \"Orthographic_Diff\", \"CharNoise_Flag\", \"Abbreviation_Match\", \"NER_Match\", \"POS_Match\"]].to_csv(\n",
        "    os.path.join(OUTPUT_PATH, \"missing_terms.csv\"), index=False)\n",
        "\n",
        "summary = {\n",
        "    \"Total Terms\": len(df),\n",
        "    \"Accepted (GMM)\": (df[\"Classification_GMM\"] == \"Acceptable\").sum(),\n",
        "    \"Divergence (GMM)\": (df[\"Classification_GMM\"] == \"Divergence\").sum(),\n",
        "    \"Accepted (Static)\": (df[\"Classification_Static\"] == \"Acceptable\").sum(),\n",
        "    \"Divergence (Static)\": (df[\"Classification_Static\"] == \"Divergence\").sum(),\n",
        "    \"Orthographic Errors\": (df[\"Orthographic_Diff\"] == \"Orthographic\").sum(),\n",
        "    \"Diacritic Errors\": (df[\"Orthographic_Diff\"] == \"Diacritic\").sum(),\n",
        "    \"CharNoise Matches\": (df[\"Orthographic_Diff\"] == \"CharNoise\").sum(),\n",
        "    \"WeirdChar Issues\": (df[\"Orthographic_Diff\"] == \"WeirdChar\").sum(),\n",
        "    \"Abbreviation Matches\": (df[\"Abbreviation_Match\"] != \"No\").sum(),\n",
        "    \"NER Drops\": (df[\"NER_Match\"] == \"NER_Drop\").sum(),\n",
        "    \"POS Mismatches\": (df[\"POS_Match\"] == \"POS_Mismatch\").sum(),\n",
        "    \"Threshold GMM\": round(thresh_gmm, 3),\n",
        "    \"Threshold Static\": STATIC_THRESHOLD\n",
        "}\n",
        "\n",
        "pd.DataFrame([summary]).to_csv(os.path.join(STATS_PATH, \"cass_summary_stats.csv\"), index=False)\n",
        "\n",
        "print(\"âœ… Ultra CASS+ Evaluation Completed\")\n",
        "print(f\"â†’ Outputs saved to: {OUTPUT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell: Evaluate CASS Score Performance (Accuracy / Precision / Recall) ===\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "# Load gold-standard if available\n",
        "gold_path = os.path.join(DATA_PATH, \"gold_labels.csv\")  # Must contain columns: Sentence_ID, True_Label (Acceptable/Divergence)\n",
        "if os.path.exists(gold_path):\n",
        "    gold_df = pd.read_csv(gold_path)\n",
        "    df_eval = pd.merge(df, gold_df, on=\"Sentence_ID\", how=\"inner\")\n",
        "\n",
        "    # Static threshold evaluation\n",
        "    y_true = df_eval[\"True_Label\"]\n",
        "    y_pred = df_eval[\"Classification_Static\"]\n",
        "\n",
        "    print(\"\\nğŸ“Š Static Threshold Evaluation:\")\n",
        "    print(\"Accuracy:\", metrics.accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", metrics.precision_score(y_true, y_pred, pos_label=\"Divergence\"))\n",
        "    print(\"Recall:\", metrics.recall_score(y_true, y_pred, pos_label=\"Divergence\"))\n",
        "    print(\"F1 Score:\", metrics.f1_score(y_true, y_pred, pos_label=\"Divergence\"))\n",
        "\n",
        "    # GMM-based evaluation\n",
        "    y_pred_gmm = df_eval[\"Classification_GMM\"]\n",
        "    print(\"\\nğŸ“Š GMM Threshold Evaluation:\")\n",
        "    print(\"Accuracy:\", metrics.accuracy_score(y_true, y_pred_gmm))\n",
        "    print(\"Precision:\", metrics.precision_score(y_true, y_pred_gmm, pos_label=\"Divergence\"))\n",
        "    print(\"Recall:\", metrics.recall_score(y_true, y_pred_gmm, pos_label=\"Divergence\"))\n",
        "    print(\"F1 Score:\", metrics.f1_score(y_true, y_pred_gmm, pos_label=\"Divergence\"))\n",
        "else:\n",
        "    print(\"âš ï¸ Gold-standard labels file not found. Please add 'gold_labels.csv' with Sentence_ID and True_Label.\")\n"
      ],
      "metadata": {
        "id": "H-M8BKeKSJtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "okGmE8hi69w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y6zXvVh46937"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 2ï¸âƒ£ Enhanced Sentence-Level Aggregation for CASS Results (with Diacritic/Ortho Analytics) ===\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# === Paths ===\n",
        "DATA_PATH = \"/content/drive/MyDrive/Summer/CASS\"\n",
        "OUTPUT_PATH = os.path.join(DATA_PATH, \"output_cass\")\n",
        "TERM_FREQ_FILE = os.path.join(OUTPUT_PATH, \"cass_missing_term_frequencies.csv\")\n",
        "SENTENCE_SUMMARY_FILE = os.path.join(OUTPUT_PATH, \"cass_sentence_level_summary.csv\")\n",
        "EXPANDED_SUMMARY_FILE = os.path.join(OUTPUT_PATH, \"cass_expanded_term_summary.csv\")\n",
        "DIVERGENCE_REPORT = os.path.join(OUTPUT_PATH, \"cass_divergence_breakdown.csv\")\n",
        "\n",
        "# === Threshold Logic ===\n",
        "FALLBACK_THRESHOLD = 0.62\n",
        "stats_path = os.path.join(OUTPUT_PATH, \"stats/cass_summary_stats.csv\")\n",
        "if os.path.exists(stats_path):\n",
        "    stats_df = pd.read_csv(stats_path)\n",
        "    if \"Threshold GMM\" in stats_df.columns:\n",
        "        CLASSIFICATION_THRESHOLD = stats_df[\"Threshold GMM\"].iloc[0]\n",
        "    else:\n",
        "        CLASSIFICATION_THRESHOLD = FALLBACK_THRESHOLD\n",
        "else:\n",
        "    CLASSIFICATION_THRESHOLD = FALLBACK_THRESHOLD\n",
        "\n",
        "print(f\"ğŸ”§ Using classification threshold: {CLASSIFICATION_THRESHOLD:.4f}\")\n",
        "\n",
        "# === Load Data ===\n",
        "df = pd.read_csv(os.path.join(OUTPUT_PATH, \"mt_eval_CASS.csv\"))\n",
        "\n",
        "# === Enhanced Classification ===\n",
        "if 'Classification_Static' in df.columns:\n",
        "    df['Missing_Term_Classification'] = df['Classification_Static']\n",
        "else:\n",
        "    df['Missing_Term_Classification'] = np.where(\n",
        "        df['CASS_Score'] >= CLASSIFICATION_THRESHOLD,\n",
        "        'Acceptable',\n",
        "        'Divergence'\n",
        "    )\n",
        "\n",
        "# === Term Frequency (Excluding Non-English) ===\n",
        "valid_terms = df[df['Missing_Term'] != \"â€”\"]\n",
        "term_freq = valid_terms['Missing_Term'].value_counts().reset_index()\n",
        "term_freq.columns = ['Missing_Term', 'Frequency']\n",
        "term_freq = term_freq.merge(\n",
        "    valid_terms.groupby('Missing_Term')['CASS_Score'].mean().reset_index(),\n",
        "    on='Missing_Term',\n",
        "    how='left'\n",
        ").rename(columns={'CASS_Score': 'Avg_CASS'})\n",
        "term_freq['Divergence_Rate'] = valid_terms.groupby('Missing_Term')['Missing_Term_Classification'].apply(\n",
        "    lambda x: (x == 'Divergence').mean()\n",
        ").reset_index()['Missing_Term_Classification']\n",
        "term_freq.to_csv(TERM_FREQ_FILE, index=False)\n",
        "\n",
        "# === Enhanced Explanation Flags ===\n",
        "def generate_explanation(row):\n",
        "    reasons = []\n",
        "    if 'NonEnglish' in str(row.get('Explanation_Static', '')):\n",
        "        return \"NonEnglish\"\n",
        "    sim = row.get('Similarity_Score', 0)\n",
        "    if sim < 0.4:\n",
        "        reasons.append(f\"LowSim({sim:.2f})\")\n",
        "    if row.get('Conflict_Match', 0) == 0:\n",
        "        reasons.append(\"NoLexMatch\")\n",
        "    ortho = row.get('Orthographic_Diff', 'No')\n",
        "    if ortho == \"Diacritic\":\n",
        "        reasons.append(\"DiacriticShift\")\n",
        "    elif ortho == \"Yes\":\n",
        "        reasons.append(\"OrthoDivergence\")\n",
        "    cass = row.get('CASS_Score', 0)\n",
        "    if cass < CLASSIFICATION_THRESHOLD:\n",
        "        reasons.append(f\"Threshold({cass:.2f}<{CLASSIFICATION_THRESHOLD:.2f})\")\n",
        "    return \"|\".join(reasons) if reasons else \"AllGood\"\n",
        "\n",
        "df['Explanation_Flag'] = df.apply(generate_explanation, axis=1)\n",
        "\n",
        "# === Save Term-Level Expanded Output ===\n",
        "term_cols = [\n",
        "    'Sentence_ID', 'Original_EN', 'MT_EN', 'Missing_Term',\n",
        "    'Best_Candidate', 'CASS_Score', 'Similarity_Score',\n",
        "    'Conflict_Match', 'Orthographic_Diff', 'Missing_Term_Classification',\n",
        "    'Explanation_Flag'\n",
        "]\n",
        "if 'MT_Lang' in df.columns:\n",
        "    term_cols.append('MT_Lang')\n",
        "df[term_cols].to_csv(EXPANDED_SUMMARY_FILE, index=False)\n",
        "\n",
        "# === Robust Sentence-Level Aggregation (with Ortho/Diacritic Error Breakdown) ===\n",
        "df['Is_Divergence'] = df['Missing_Term_Classification'] == 'Divergence'\n",
        "df['Is_Ortho_Error'] = df['Orthographic_Diff'] == 'Yes'\n",
        "df['Is_Diacritic_Error'] = df['Orthographic_Diff'] == 'Diacritic'\n",
        "df['Is_Lex_Mismatch'] = df['Conflict_Match'] == 0\n",
        "df['Is_Low_Similarity'] = df['Similarity_Score'] < 0.4\n",
        "\n",
        "agg_config = {\n",
        "    'Original_EN': 'first',\n",
        "    'MT_EN': 'first',\n",
        "    'CASS_Score': ['count', 'min', 'max', 'mean'],\n",
        "    'Is_Divergence': 'sum',\n",
        "    'Is_Ortho_Error': 'sum',\n",
        "    'Is_Diacritic_Error': 'sum',\n",
        "    'Is_Lex_Mismatch': 'sum',\n",
        "    'Is_Low_Similarity': 'sum',\n",
        "    'Missing_Term_Classification': lambda x: '|'.join(x),\n",
        "    'Explanation_Flag': lambda x: '||'.join(x)\n",
        "}\n",
        "if 'MT_Lang' in df.columns:\n",
        "    agg_config['MT_Lang'] = 'first'\n",
        "\n",
        "sentence_summary = df.groupby('Sentence_ID').agg(agg_config).reset_index()\n",
        "sentence_summary.columns = [\n",
        "    'Sentence_ID',\n",
        "    'Original_EN',\n",
        "    'MT_EN',\n",
        "    'Num_Missing_Terms',\n",
        "    'Min_CASS',\n",
        "    'Max_CASS',\n",
        "    'Mean_CASS',\n",
        "    'Num_Divergent_Terms',\n",
        "    'Num_Orthographic_Errors',\n",
        "    'Num_Diacritic_Errors',\n",
        "    'Num_Lexicon_Mismatches',\n",
        "    'Num_Low_Similarity',\n",
        "    'Term_Classifications',\n",
        "    'Term_Explanation_Flags'\n",
        "] + (['MT_Lang'] if 'MT_Lang' in df.columns else [])\n",
        "\n",
        "# Sentence-level labels\n",
        "sentence_summary['Sentence_Classification_Mean'] = np.where(\n",
        "    sentence_summary['Mean_CASS'] >= CLASSIFICATION_THRESHOLD,\n",
        "    'Acceptable',\n",
        "    'Divergence'\n",
        ")\n",
        "sentence_summary['Sentence_Classification_Strict'] = np.where(\n",
        "    sentence_summary['Num_Divergent_Terms'] > 0,\n",
        "    'Divergence',\n",
        "    'Acceptable'\n",
        ")\n",
        "sentence_summary['Divergence_Severity'] = np.where(\n",
        "    sentence_summary['Mean_CASS'] < CLASSIFICATION_THRESHOLD - 0.2,\n",
        "    'High',\n",
        "    np.where(sentence_summary['Mean_CASS'] < CLASSIFICATION_THRESHOLD, 'Medium', 'None')\n",
        ")\n",
        "\n",
        "sentence_summary.to_csv(SENTENCE_SUMMARY_FILE, index=False)\n",
        "\n",
        "# === Divergence Analytics Report (With Explicit Column Names) ===\n",
        "divergence_terms = df[df['Missing_Term_Classification'] == 'Divergence'][[\n",
        "    'Sentence_ID', 'Original_EN', 'MT_EN', 'Missing_Term',\n",
        "    'Best_Candidate', 'CASS_Score', 'Similarity_Score', 'Conflict_Match',\n",
        "    'Orthographic_Diff', 'Explanation_Flag'\n",
        "] + (['MT_Lang'] if 'MT_Lang' in df.columns else [])]\n",
        "\n",
        "divergence_terms.to_csv(DIVERGENCE_REPORT, index=False)\n",
        "\n",
        "# === Final Output ===\n",
        "print(\"\\nâœ… Enhanced CASS Aggregation Completed\")\n",
        "print(f\"â†’ Classified {len(df)} term instances\")\n",
        "print(f\"â†’ Analyzed {len(sentence_summary)} sentences\")\n",
        "print(f\"â†’ Divergence rate: {sentence_summary['Num_Divergent_Terms'].sum()/len(df):.1%}\")\n",
        "print(f\"â†’ Orthographic errors: {df['Orthographic_Diff'].eq('Yes').sum()} terms\")\n",
        "print(f\"â†’ Diacritic errors: {df['Orthographic_Diff'].eq('Diacritic').sum()} terms\")\n",
        "print(f\"â†’ Output files saved to {OUTPUT_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PTVIYexS5Cf",
        "outputId": "d33357fa-0cd3-450f-fd1f-c22e3ca1853d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Using classification threshold: 0.7260\n",
            "\n",
            "âœ… Enhanced CASS Aggregation Completed\n",
            "â†’ Classified 331 term instances\n",
            "â†’ Analyzed 277 sentences\n",
            "â†’ Divergence rate: 85.8%\n",
            "â†’ Orthographic errors: 0 terms\n",
            "â†’ Diacritic errors: 11 terms\n",
            "â†’ Output files saved to /content/drive/MyDrive/Summer/CASS/output_cass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 3ï¸âƒ£ Enhanced CASS Visualization & Summary Script (Palette/Hue Fix) ===\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "# === Paths ===\n",
        "DATA_PATH = \"/content/drive/MyDrive/Summer/CASS\"\n",
        "OUTPUT_PATH = os.path.join(DATA_PATH, \"output_cass\")\n",
        "STATS_PATH = os.path.join(OUTPUT_PATH, \"stats\")\n",
        "os.makedirs(STATS_PATH, exist_ok=True)\n",
        "\n",
        "# === Load Data ===\n",
        "df_terms = pd.read_csv(os.path.join(OUTPUT_PATH, \"cass_expanded_term_summary.csv\"))\n",
        "df_sentences = pd.read_csv(os.path.join(OUTPUT_PATH, \"cass_sentence_level_summary.csv\"))\n",
        "term_freq = pd.read_csv(os.path.join(OUTPUT_PATH, \"cass_missing_term_frequencies.csv\"))\n",
        "\n",
        "# === Term-Level CASS Score Distribution ===\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_terms['CASS_Score'], bins=20, kde=True, color=\"skyblue\")\n",
        "plt.axvline(0.62, color=\"red\", linestyle=\"--\", label=\"Static Threshold = 0.62\")\n",
        "plt.title(\"Term-Level CASS Score Distribution\")\n",
        "plt.xlabel(\"CASS Score\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(STATS_PATH, \"term_level_cass_distribution.png\"))\n",
        "plt.close()\n",
        "\n",
        "# === Sentence-Level Mean CASS Distribution ===\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_sentences['Mean_CASS'], bins=20, kde=True, color=\"orange\")\n",
        "plt.axvline(0.62, color=\"red\", linestyle=\"--\", label=\"Static Threshold = 0.62\")\n",
        "plt.title(\"Sentence-Level Mean CASS Score Distribution\")\n",
        "plt.xlabel(\"Mean CASS Score\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(STATS_PATH, \"sentence_level_cass_distribution.png\"))\n",
        "plt.close()\n",
        "\n",
        "# === Sentence-Level Classification Bar Chart ===\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.countplot(\n",
        "    data=df_sentences,\n",
        "    x=\"Sentence_Classification_Mean\",\n",
        "    hue=\"Sentence_Classification_Mean\",\n",
        "    palette=\"Set2\",\n",
        "    legend=False\n",
        ")\n",
        "plt.title(\"Sentence-Level Classification (Acceptable vs Divergence)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(STATS_PATH, \"sentence_level_classification.png\"))\n",
        "plt.close()\n",
        "\n",
        "# === Term-Level Classification Bar Chart ===\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.countplot(\n",
        "    data=df_terms,\n",
        "    x=\"Missing_Term_Classification\",\n",
        "    hue=\"Missing_Term_Classification\",\n",
        "    palette=\"coolwarm\",\n",
        "    legend=False\n",
        ")\n",
        "plt.title(\"Term-Level Classification (Acceptable vs Divergence)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(STATS_PATH, \"term_level_classification.png\"))\n",
        "plt.close()\n",
        "\n",
        "# === Orthographic/Diacritic Error Frequency Plots ===\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.countplot(\n",
        "    data=df_terms,\n",
        "    x=\"Orthographic_Diff\",\n",
        "    hue=\"Orthographic_Diff\",\n",
        "    order=[\"No\", \"Diacritic\", \"Yes\"],\n",
        "    palette=\"rocket\",\n",
        "    legend=False\n",
        ")\n",
        "plt.title(\"Orthographic & Diacritic Error Breakdown (Terms)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(STATS_PATH, \"term_level_ortho_diakritik_errors.png\"))\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "error_counts = df_terms[\"Orthographic_Diff\"].value_counts()\n",
        "sns.barplot(\n",
        "    x=error_counts.index,\n",
        "    y=error_counts.values,\n",
        "    hue=error_counts.index,\n",
        "    palette=\"rocket\",\n",
        "    legend=False\n",
        ")\n",
        "plt.title(\"Term Count by Orthographic Error Type\")\n",
        "plt.ylabel(\"Term Count\")\n",
        "plt.xlabel(\"Orthographic Error Type\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(STATS_PATH, \"ortho_error_type_counts.png\"))\n",
        "plt.close()\n",
        "\n",
        "# === Explanation Flag Frequency ===\n",
        "flag_counts = Counter(df_terms['Explanation_Flag'])\n",
        "flag_df = pd.DataFrame(flag_counts.items(), columns=[\"Explanation_Flag\", \"Count\"]).sort_values(\"Count\", ascending=False)\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(\n",
        "    data=flag_df,\n",
        "    x=\"Explanation_Flag\",\n",
        "    y=\"Count\",\n",
        "    hue=\"Explanation_Flag\",\n",
        "    palette=\"magma\",\n",
        "    legend=False\n",
        ")\n",
        "plt.title(\"Explanation Flag Frequency\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(STATS_PATH, \"explanation_flag_frequency.png\"))\n",
        "plt.close()\n",
        "\n",
        "# === Top 20 Most Missing Terms ===\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_terms = term_freq.nlargest(20, 'Frequency')\n",
        "sns.barplot(\n",
        "    data=top_terms,\n",
        "    x=\"Missing_Term\",\n",
        "    y=\"Frequency\",\n",
        "    hue=\"Missing_Term\",\n",
        "    palette=\"viridis\",\n",
        "    legend=False\n",
        ")\n",
        "plt.title(\"Top 20 Most Frequently Missing Terms\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(STATS_PATH, \"missing_term_frequency_top20.png\"))\n",
        "plt.close()\n",
        "\n",
        "# === Statistics Summary ===\n",
        "summary = {\n",
        "    \"Total Missing Terms\": len(df_terms),\n",
        "    \"Unique Missing Terms\": df_terms['Missing_Term'].nunique(),\n",
        "    \"Total Sentences Evaluated\": len(df_sentences),\n",
        "    \"Average Terms per Sentence\": round(df_terms.shape[0] / df_sentences.shape[0], 2),\n",
        "    \"Term Acceptable %\": round((df_terms['Missing_Term_Classification'] == 'Acceptable').mean() * 100, 2),\n",
        "    \"Sentence Acceptable %\": round((df_sentences['Sentence_Classification_Mean'] == 'Acceptable').mean() * 100, 2),\n",
        "    \"Term-Level Mean CASS\": round(df_terms['CASS_Score'].mean(), 4),\n",
        "    \"Sentence-Level Mean CASS\": round(df_sentences['Mean_CASS'].mean(), 4),\n",
        "    \"Diacritic Error Count\": df_terms[\"Orthographic_Diff\"].eq(\"Diacritic\").sum(),\n",
        "    \"Orthographic Error Count\": df_terms[\"Orthographic_Diff\"].eq(\"Yes\").sum(),\n",
        "    \"No Error Count\": df_terms[\"Orthographic_Diff\"].eq(\"No\").sum(),\n",
        "}\n",
        "pd.DataFrame([summary]).to_csv(os.path.join(STATS_PATH, \"cass_summary_report.csv\"), index=False)\n",
        "\n",
        "print(\"âœ… All CASS visualizations and summary report saved to:\")\n",
        "print(f\"â†’ {STATS_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v8n6ItG1_kX",
        "outputId": "ef4d909d-693c-4305-c06f-33f3af46ef41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All CASS visualizations and summary report saved to:\n",
            "â†’ /content/drive/MyDrive/Summer/CASS/output_cass/stats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "31cMG2zLr-4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vhqbdmXmKTAZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3de94d87ea24c1696136b571abc9c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24e4ca6f90ce426398e01ac370a29dcc",
              "IPY_MODEL_8b7f64574bb7412f9cef3a4b55f1692d",
              "IPY_MODEL_0c0264d8396447e89442fcef992cc4af"
            ],
            "layout": "IPY_MODEL_93d44555d89043b49a3858eb5386dbbc"
          }
        },
        "24e4ca6f90ce426398e01ac370a29dcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db85b87b84e34604be2af2a049d1bba2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7fa2ff4d57ba4bf38a99232d81deae60",
            "value": "Downloadingâ€‡https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:â€‡"
          }
        },
        "8b7f64574bb7412f9cef3a4b55f1692d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e956b99f5e50409d9906e89fb5e7e5c5",
            "max": 53069,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98750e48bb2046bba4765b93613d4806",
            "value": 53069
          }
        },
        "0c0264d8396447e89442fcef992cc4af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbb3e63c6f734c9e9a98cc0452ec850e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2e1a43e6ec4c4aad936882613a28a932",
            "value": "â€‡428k/?â€‡[00:00&lt;00:00,â€‡30.3MB/s]"
          }
        },
        "93d44555d89043b49a3858eb5386dbbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db85b87b84e34604be2af2a049d1bba2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fa2ff4d57ba4bf38a99232d81deae60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e956b99f5e50409d9906e89fb5e7e5c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98750e48bb2046bba4765b93613d4806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dbb3e63c6f734c9e9a98cc0452ec850e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e1a43e6ec4c4aad936882613a28a932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eab3c380ceb640c8b087aeb160855a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbabfa6db9b84e7ebf9b99670551bc0d",
              "IPY_MODEL_aef873bcade541969f90b8e8fe6eef41",
              "IPY_MODEL_5d0f3e6ef7334805a6c673a8ffd09f03"
            ],
            "layout": "IPY_MODEL_3878eb096b0a4ec7b316a6a5684721f4"
          }
        },
        "dbabfa6db9b84e7ebf9b99670551bc0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54d7304592b94e108444ecd44dcc8605",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_40a8277c15844c61b769ff4077d23b51",
            "value": "Downloadingâ€‡https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:â€‡"
          }
        },
        "aef873bcade541969f90b8e8fe6eef41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_484e0b654ecb4654b8019eb2b7c06585",
            "max": 53069,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b86f09fb04ce40b6a81d44d778c1e938",
            "value": 53069
          }
        },
        "5d0f3e6ef7334805a6c673a8ffd09f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_444c42a60bf44725a8e4259731fb9d4f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e17ce0e342f24219a736c9090597faa9",
            "value": "â€‡428k/?â€‡[00:00&lt;00:00,â€‡17.7MB/s]"
          }
        },
        "3878eb096b0a4ec7b316a6a5684721f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54d7304592b94e108444ecd44dcc8605": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40a8277c15844c61b769ff4077d23b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "484e0b654ecb4654b8019eb2b7c06585": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b86f09fb04ce40b6a81d44d778c1e938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "444c42a60bf44725a8e4259731fb9d4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e17ce0e342f24219a736c9090597faa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}